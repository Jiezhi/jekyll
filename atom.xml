<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>介之的博客</title>
  
  <subtitle>好记性不如烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jiezhi.github.io/"/>
  <updated>2020-03-31T07:56:09.795Z</updated>
  <id>http://jiezhi.github.io/</id>
  
  <author>
    <name>Jiezhi.G</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CDH6配置 Atlas，及 Hive Hook</title>
    <link href="http://jiezhi.github.io/2020/03/31/atlas2-cdh6-hive-hook/"/>
    <id>http://jiezhi.github.io/2020/03/31/atlas2-cdh6-hive-hook/</id>
    <published>2020-03-31T07:03:57.000Z</published>
    <updated>2020-03-31T07:56:09.795Z</updated>
    
    <content type="html"><![CDATA[<p>因为 CDH 社区版不能使用 <a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/datamgmt_intro.html#intro_to_cloudera_navigator_data_mgmt" target="_blank" rel="noopener">Navigator</a>，所以需要自己集成一个<a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/datamgmt_intro.html#intro_to_cloudera_navigator_data_mgmt" target="_blank" rel="noopener">Apache Atlas</a>。</p><a id="more"></a><h1 id="版本说明"><a href="#版本说明" class="headerlink" title="版本说明"></a>版本说明</h1><p>Atlas: 2.0 <a href="https://atlas.apache.org/#/Downloads" target="_blank" rel="noopener">Download</a></p><p>CDH: 6.3.1 (Parcels)</p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>Atlas 依赖 Solr（或 ES）、HBase和 Kafka 来工作，先确保 CDH 这3个已经开启，或者编译 Atlas代码时集成进去（生产不建议）。</p><h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><p>从官网下载Atlas 2.0的代码，然后进行编译，详见官网（基本没啥坑，除非 maven 版本太低，去下一个最新的即可）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar xvfz apache-atlas-2.0.0-sources.tar.gz</span><br><span class="line"><span class="built_in">cd</span> apache-atlas-sources-2.0.0/</span><br><span class="line"><span class="built_in">export</span> MAVEN_OPTS=<span class="string">"-Xms2g -Xmx2g"</span></span><br><span class="line">mvn clean -DskipTests install</span><br><span class="line">mvn clean -DskipTests package -Pdist</span><br></pre></td></tr></table></figure></p><p>编译好的位置在<code>distro/target/</code>下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 target]<span class="comment"># ll distro/target</span></span><br><span class="line">total 738M</span><br><span class="line">drwxr-xr-x. 3 root root   32 Mar 31 10:08 apache-atlas-2.0.0-bin</span><br><span class="line">-rw-r--r--. 1 root root 360M Mar 31 10:08 apache-atlas-2.0.0-bin.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   44 Mar 31 10:08 apache-atlas-2.0.0-falcon-hook</span><br><span class="line">-rw-r--r--. 1 root root 8.8M Mar 31 10:08 apache-atlas-2.0.0-falcon-hook.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   43 Mar 31 10:08 apache-atlas-2.0.0-hbase-hook</span><br><span class="line">-rw-r--r--. 1 root root  11M Mar 31 10:08 apache-atlas-2.0.0-hbase-hook.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   42 Mar 31 10:08 apache-atlas-2.0.0-hive-hook</span><br><span class="line">-rw-r--r--. 1 root root  16M Mar 31 10:08 apache-atlas-2.0.0-hive-hook.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   43 Mar 31 10:08 apache-atlas-2.0.0-kafka-hook</span><br><span class="line">-rw-r--r--. 1 root root 8.8M Mar 31 10:08 apache-atlas-2.0.0-kafka-hook.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   32 Mar 31 10:08 apache-atlas-2.0.0-server</span><br><span class="line">-rw-r--r--. 1 root root 260M Mar 31 10:08 apache-atlas-2.0.0-server.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root  11M Mar 31 10:08 apache-atlas-2.0.0-sources.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   43 Mar 31 10:08 apache-atlas-2.0.0-sqoop-hook</span><br><span class="line">-rw-r--r--. 1 root root 8.8M Mar 31 10:08 apache-atlas-2.0.0-sqoop-hook.tar.gz</span><br><span class="line">drwxr-xr-x. 3 root root   43 Mar 31 10:08 apache-atlas-2.0.0-storm-hook</span><br><span class="line">-rw-r--r--. 1 root root  57M Mar 31 10:08 apache-atlas-2.0.0-storm-hook.tar.gz</span><br><span class="line">drwxr-xr-x. 2 root root    6 Mar 31 10:08 archive-tmp</span><br><span class="line">-rw-r--r--. 1 root root  94K Mar 31 10:08 atlas-distro-2.0.0.jar</span><br><span class="line">drwxr-xr-x. 2 root root 4.0K Mar 31 10:08 bin</span><br><span class="line">drwxr-xr-x. 5 root root  231 Mar 31 10:08 conf</span><br><span class="line">drwxr-xr-x. 2 root root   28 Mar 31 10:08 maven-archiver</span><br><span class="line">drwxr-xr-x. 3 root root   22 Mar 31 10:08 maven-shared-archive-resources</span><br><span class="line">drwxr-xr-x. 2 root root   55 Mar 31 10:08 META-INF</span><br><span class="line">-rw-r--r--. 1 root root 3.9K Mar 31 10:08 rat.txt</span><br><span class="line">drwxr-xr-x. 3 root root   22 Mar 31 10:08 <span class="built_in">test</span>-classes</span><br></pre></td></tr></table></figure></p><p>我们直接使用<code>apache-atlas-2.0.0-bin.tar.gz</code>和需要 hook 的组件如：<code>apache-atlas-2.0.0-hive-hook.tar.gz</code> 、<code>apache-atlas-2.0.0-sqoop-hook.tar.gz</code></p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>将需要的 tar.gz文件解压到指定目录如<code>/opt/atlas</code></p><p>改几个关键的配置<code>vim conf/atlas-application.properties</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">atlas.graph.storage.hostname=node1:2181,node2:2181,node3:2181</span><br><span class="line"></span><br><span class="line"><span class="comment">#Solr</span></span><br><span class="line"><span class="comment">#Solr cloud mode properties</span></span><br><span class="line">atlas.graph.index.search.solr.mode=cloud</span><br><span class="line">atlas.graph.index.search.solr.zookeeper-url=node1:2181/solr,node2:2181/solr,node3:2181/solr</span><br><span class="line">atlas.graph.index.search.solr.zookeeper-connect-timeout=60000</span><br><span class="line">atlas.graph.index.search.solr.zookeeper-session-timeout=60000</span><br><span class="line">atlas.graph.index.search.solr.wait-searcher=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#########  Notification Configs  #########</span></span><br><span class="line">atlas.notification.embedded=<span class="literal">false</span></span><br><span class="line">atlas.kafka.data=<span class="variable">$&#123;sys:atlas.home&#125;</span>/data/kafka</span><br><span class="line">atlas.kafka.zookeeper.connect=node1:2181,node2:2181,node3:2181</span><br><span class="line">atlas.kafka.bootstrap.servers=node1:9092,node2:9092,node3:9092</span><br><span class="line">atlas.kafka.zookeeper.session.timeout.ms=4000</span><br><span class="line">atlas.kafka.zookeeper.connection.timeout.ms=2000</span><br><span class="line">atlas.kafka.zookeeper.sync.time.ms=20</span><br><span class="line">atlas.kafka.auto.commit.interval.ms=1000</span><br><span class="line">atlas.kafka.hook.group.id=atlas</span><br><span class="line"></span><br><span class="line"><span class="comment">#########  Server Properties  #########</span></span><br><span class="line">atlas.rest.address=http://0.0.0.0:21001</span><br><span class="line"></span><br><span class="line"><span class="comment">#########  Entity Audit Configs  #########</span></span><br><span class="line">atlas.audit.hbase.tablename=apache_atlas_entity_audit</span><br><span class="line">atlas.audit.zookeeper.session.timeout.ms=1000</span><br><span class="line">atlas.audit.hbase.zookeeper.quorum=node1:2181,node2:2181,node3:2181</span><br><span class="line"></span><br><span class="line"><span class="comment">## Server port configuration （这边使用210001，避免端口和 Impala 的冲突）</span></span><br><span class="line">atlas.server.http.port=21001</span><br><span class="line"><span class="comment">#atlas.server.https.port=21443</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 以下为新增配置，按需添加</span></span><br><span class="line"><span class="comment">######### Hive Hook Configs #######</span></span><br><span class="line"></span><br><span class="line">atlas.hook.hive.synchronous=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">atlas.hook.hive.numRetries=3</span><br><span class="line"></span><br><span class="line">atlas.hook.hive.queueSize=10000</span><br><span class="line"></span><br><span class="line">atlas.cluster.name=primary</span><br><span class="line"></span><br><span class="line"><span class="comment">######### Sqoop Hook Configs #######</span></span><br><span class="line">atlas.hook.sqoop.synchronous=<span class="literal">false</span> <span class="comment"># whether to run the hook synchronously. false recommended to avoid delays in Sqoop operation completion. Default: false</span></span><br><span class="line">atlas.hook.sqoop.numRetries=3      <span class="comment"># number of retries for notification failure. Default: 3</span></span><br><span class="line">atlas.hook.sqoop.queueSize=10000   <span class="comment"># queue size for the threadpool. Default: 10000</span></span><br></pre></td></tr></table></figure><p>这时候可以尝试启动下<code>./bin/atlas_start.py</code>并看一下 log <code>tail -f logs/application.log</code>，我这边遇到了问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2020-03-31 11:42:57,437 INFO  - [main:] ~ Creating indexes <span class="keyword">for</span> graph. (GraphBackedS</span><br><span class="line">earchIndexer:248)</span><br><span class="line">2020-03-31 11:42:58,605 INFO  - [main:] ~ Created index : vertex_index (GraphBacked</span><br><span class="line">SearchIndexer:253)</span><br><span class="line">2020-03-31 11:42:58,692 INFO  - [main:] ~ Created index : edge_index (GraphBackedSe</span><br><span class="line">archIndexer:259)</span><br><span class="line">2020-03-31 11:42:58,700 INFO  - [main:] ~ Created index : fulltext_index (GraphBackedSearchIndexer:265)</span><br><span class="line">2020-03-31 11:42:58,824 ERROR - [main:] ~ GraphBackedSearchIndexer.initialize() failed (GraphBackedSearchIndexer:307)</span><br><span class="line">org.apache.solr.client.solrj.impl.HttpSolrClient<span class="variable">$RemoteSolrException</span>: Error from server at http://BD-Cal-Pro-02:8983/solr: Can not find the specified config <span class="built_in">set</span>: vertex_index</span><br><span class="line">        at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:627)</span><br><span class="line">        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:253)</span><br></pre></td></tr></table></figure></p><p>不知道为何 atlas 没能在 solr 创建需要的数据，只能手动创建了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">solrctl collection --create vertex_index -s 1 -c atlas -r 1</span><br><span class="line"></span><br><span class="line">solrctl collection --create edge_index -s 1 -c atlas -r 1</span><br><span class="line"></span><br><span class="line">solrctl collection --create fulltext_index -s 1 -c atlas -r 1</span><br></pre></td></tr></table></figure></p><p>这个其实我也加不了，只好用 solr web 页面加好了。</p><p>至此 atlas 就可以正常启动了。</p><h2 id="Jar-包配置"><a href="#Jar-包配置" class="headerlink" title="Jar 包配置"></a>Jar 包配置</h2><p>把配置复制到 hook/hive 下，然后打包进atlas-plugin-classloader-2.0.0.jar</p><p><code>zip -u atlas-plugin-classloader-2.0.0.jar atlas-application.properties</code></p><p>有的教程是直接跨目录压缩进去了，但实际执行会出错。</p><h1 id="Hive-Hook"><a href="#Hive-Hook" class="headerlink" title="Hive Hook"></a>Hive Hook</h1><p>这边需要去CM 中 Hive 配置页面修改：</p><h3 id="Hive-Auxiliary-JARs-Directory"><a href="#Hive-Auxiliary-JARs-Directory" class="headerlink" title="Hive Auxiliary JARs Directory"></a>Hive Auxiliary JARs Directory</h3><blockquote><p>${ATLAS_HOME}/hook/hive</p></blockquote><h3 id="Gateway-Client-Environment-Advanced-Configuration-Snippet-Safety-Valve-for-hive-env-sh"><a href="#Gateway-Client-Environment-Advanced-Configuration-Snippet-Safety-Valve-for-hive-env-sh" class="headerlink" title="Gateway Client Environment Advanced Configuration Snippet (Safety Valve) for hive-env.sh"></a>Gateway Client Environment Advanced Configuration Snippet (Safety Valve) for <a href="http://hive-env.sh/" target="_blank" rel="noopener">hive-env.sh</a></h3><blockquote><p>HIVE_AUX_JARS_PATH=${ATLAS_HOME}/hook/hive</p></blockquote><h3 id="HiveServer2-Advanced-Configuration-Snippet-Safety-Valve-for-hive-site-xml"><a href="#HiveServer2-Advanced-Configuration-Snippet-Safety-Valve-for-hive-site-xml" class="headerlink" title="HiveServer2 Advanced Configuration Snippet (Safety Valve) for hive-site.xml"></a>HiveServer2 Advanced Configuration Snippet (Safety Valve) for hive-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.exec.post.hooks&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.reloadable.aux.jars.path&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="variable">$&#123;ATLAS_HOME&#125;</span>/hook/hive/&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;atlas.cluster.name&lt;/name&gt;</span><br><span class="line">&lt;value&gt;primary&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置好后重启 Hive，创建一张测试表可以看到Atlas 中会出现该表的记录。</p><h2 id="导入历史数据"><a href="#导入历史数据" class="headerlink" title="导入历史数据"></a>导入历史数据</h2><p>如果需要历史数据，则可以通过 hook-bin下面的 import_hive.sh 导入即可，主要要把 atlas-application.properties 复制到 HIVE_CONF_HOME 下面（/etc/hive/conf）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为 CDH 社区版不能使用 &lt;a href=&quot;https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/datamgmt_intro.html#intro_to_cloudera_navigator_data_mgmt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Navigator&lt;/a&gt;，所以需要自己集成一个&lt;a href=&quot;https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/datamgmt_intro.html#intro_to_cloudera_navigator_data_mgmt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Atlas&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://jiezhi.github.io/categories/BigData/"/>
    
    
      <category term="CDH" scheme="http://jiezhi.github.io/tags/CDH/"/>
    
      <category term="Hive" scheme="http://jiezhi.github.io/tags/Hive/"/>
    
      <category term="Atlas" scheme="http://jiezhi.github.io/tags/Atlas/"/>
    
  </entry>
  
  <entry>
    <title>CDH中sqoop无法在 hue 中正确执行的问题</title>
    <link href="http://jiezhi.github.io/2020/01/17/run-sqoop-with-oozie-in-hue/"/>
    <id>http://jiezhi.github.io/2020/01/17/run-sqoop-with-oozie-in-hue/</id>
    <published>2020-01-17T09:38:36.000Z</published>
    <updated>2020-01-17T10:24:19.694Z</updated>
    
    <content type="html"><![CDATA[<p>JDBC的配置给你带来多少的麻烦？</p><a id="more"></a><p>安装好 CDH 后，添加完 sqoop 1服务后，去 hue 中执行直接报错。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">No child hadoop job is executed.</span><br><span class="line">java.lang.reflect.InvocationTargetException</span><br><span class="line">  ...</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1875</span>)</span><br><span class="line">at org.apache.oozie.action.hadoop.LauncherAM.main(LauncherAM.java:<span class="number">141</span>)</span><br><span class="line">Caused by: java.lang.SecurityException: Intercepted System.exit(<span class="number">1</span>)</span><br><span class="line">at org.apache.oozie.action.hadoop.security.LauncherSecurityManager.checkExit(LauncherSecurityManager.java:<span class="number">57</span>)</span><br><span class="line">at java.lang.Runtime.exit(Runtime.java:<span class="number">107</span>)</span><br><span class="line">at java.lang.System.exit(System.java:<span class="number">971</span>)</span><br><span class="line">at org.apache.sqoop.Sqoop.main(Sqoop.java:<span class="number">252</span>)</span><br><span class="line">at org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:<span class="number">214</span>)</span><br><span class="line">at org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:<span class="number">199</span>)</span><br><span class="line">at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:<span class="number">104</span>)</span><br><span class="line">at org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:<span class="number">51</span>)</span><br><span class="line">... <span class="number">16</span> more</span><br><span class="line">Intercepting System.exit(<span class="number">1</span>)</span><br><span class="line">Failing Oozie Launcher, Main Class [org.apache.oozie.action.hadoop.SqoopMain], exit code [<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>围绕这个问题找了很久的都无法找到对应的问题，之前都是直接去 YARN 里看的 Log。这两天去看了 stderr 里的错误才发现是 JDBC 驱动造成的问题（吐槽下某骨文公司）很多程序本可以开箱即用或者一键安装的，结果老是因为 JDBC 驱动要去手动下载而浪费很多时间。<br>再看下真正的错误：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: Could not load db driver class: com.mysql.jdbc.Driver</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:<span class="number">874</span>)</span><br><span class="line">at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:<span class="number">59</span>)</span><br><span class="line">at org.apache.sqoop.manager.CatalogQueryManager.listTables(CatalogQueryManager.java:<span class="number">102</span>)</span><br><span class="line">at org.apache.sqoop.tool.ListTablesTool.run(ListTablesTool.java:<span class="number">49</span>)</span><br><span class="line">at org.apache.sqoop.Sqoop.run(Sqoop.java:<span class="number">146</span>)</span><br></pre></td></tr></table></figure></p><p>然后以为是 sqoop JDBC 驱动没安装好，不过在节点上却可以直接用 sqoop 命令来导入导出数据。那说明 sqoop 的驱动没问题呀，我们知道 sqoop 最终是落在 oozie 上执行的，那么是不是有可能 oozie 找不到对应的驱动造成的呢。</p><p>我们去 oozie 所在的节点上执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oozie admin -shareliblist</span></span><br><span class="line">[Available ShareLib]</span><br><span class="line">hive</span><br><span class="line">distcp</span><br><span class="line">git</span><br><span class="line">mapreduce-streaming</span><br><span class="line">spark</span><br><span class="line">oozie</span><br><span class="line">hcatalog</span><br><span class="line">hive2</span><br><span class="line">sqoop</span><br><span class="line">pig</span><br></pre></td></tr></table></figure><p>看一下 sqoop 相关的 jar 包：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oozie admin -shareliblist sqoop</span></span><br><span class="line">[Available ShareLib]</span><br><span class="line">sqoop</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/HikariCP-2.6.1.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/HikariCP-java7-2.4.12.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/ST4-4.0.4.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/aggdesigner-algorithm-6.0.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/ant-1.9.1.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/ant-launcher-1.9.1.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/antlr-2.7.7.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/antlr-runtime-3.4.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/aopalliance-1.0.jar</span><br><span class="line">hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149/sqoop/aopalliance-repackaged-2.5.0-b32.jar</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p><p> grep 一下，发现并没有 JDBC 驱动，那么需要将 sqoop 目录下（一般为/var/lib/sqoop/）的 jdbc 驱动放到 oozie 目录下（具体在 HDFS 上的目录看上面的输出结果就知道了）。如果 sqoop 的 jdbc 还没配好，直接按照<a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_mc_sqoop1_client.html#topic_13_7" target="_blank" rel="noopener">CDH 文档</a>安装即可。</p><p>最后执行下更新即可：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oozie admin -sharelibupdate</span></span><br><span class="line">[ShareLib update status]</span><br><span class="line">sharelibDirOld = hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149</span><br><span class="line">host = http://yournodename:11000/oozie</span><br><span class="line">sharelibDirNew = hdfs://yournodename:8020/user/oozie/share/lib/lib_20200108192149</span><br><span class="line">status = Successful</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;JDBC的配置给你带来多少的麻烦？&lt;/p&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://jiezhi.github.io/categories/BigData/"/>
    
    
      <category term="CDH" scheme="http://jiezhi.github.io/tags/CDH/"/>
    
      <category term="Sqoop" scheme="http://jiezhi.github.io/tags/Sqoop/"/>
    
      <category term="Hue" scheme="http://jiezhi.github.io/tags/Hue/"/>
    
      <category term="Ooozie" scheme="http://jiezhi.github.io/tags/Ooozie/"/>
    
  </entry>
  
  <entry>
    <title>Supervisord介绍与使用</title>
    <link href="http://jiezhi.github.io/2020/01/16/supervisord/"/>
    <id>http://jiezhi.github.io/2020/01/16/supervisord/</id>
    <published>2020-01-16T07:19:46.000Z</published>
    <updated>2020-01-16T08:51:00.872Z</updated>
    
    <content type="html"><![CDATA[<p>有时候我们希望有些进程能时刻运行，并能在出错后自动重启。除了用系统带的 systemd 之外，你还可以用supervisord。<br>这里简单介绍 supervisord 的安装和使用。<br><a id="more"></a><br><a href="http://supervisord.org/index.html" target="_blank" rel="noopener">项目地址</a></p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装比较简单：<br><code>pip install supervisor</code></p><h1 id="创建配置文件"><a href="#创建配置文件" class="headerlink" title="创建配置文件"></a>创建配置文件</h1><p>使用<code>echo_supervisord_conf</code>命令可以创建一份配置文件模板，比如要把配置文件放在<code>/etc/supervisord.conf</code>，则执行：<br><code>echo_supervisord_conf &gt; /etc/supervisord.conf</code><br>另外如果你想把配置文件放在其他地方，则可以自己指定地址，只是在启动 supervisord 时加<code>-c</code>显式指定配置文件地址<code>supervisord -c supervisord.conf</code> </p><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h2 id="配置任务"><a href="#配置任务" class="headerlink" title="配置任务"></a>配置任务</h2><p>修改<code>supervisord.conf</code>文件，在84行左右位置加入你想要执行的进程，按如下格式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[program:theprogramname]</span><br><span class="line"><span class="built_in">command</span>=/bin/cat              ; the program (relative uses PATH, can take args)</span><br></pre></td></tr></table></figure></p><p>比如我想用命令通过 ssh 来代理远程的一个 web 页面：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[program:cm]</span><br><span class="line"><span class="built_in">command</span>=ssh -N -L *:7180:localhost:7180 214</span><br></pre></td></tr></table></figure></p><h2 id="配置-http-服务"><a href="#配置-http-服务" class="headerlink" title="配置 http 服务"></a>配置 http 服务</h2><p>supervisord 可以开启 http 服务，通过 web 页面来查看和管理任务。<br>如果需要开启，可以打开<code>inet_http_server</code>配置，大概在39行。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[inet_http_server]         ; inet (TCP) server disabled by default</span><br><span class="line">port=0.0.0.0:9001          ; ip_address:port specifier, *:port <span class="keyword">for</span> all iface</span><br><span class="line">username=admin             ; default is no username (open server)</span><br><span class="line">password=thepassword       ; default is no password (open server)</span><br></pre></td></tr></table></figure></p><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><p>直接执行<code>supervisord</code>即可启动，如果需要指定配置文件位置则使用<code>-c</code>参数。</p><h1 id="管理"><a href="#管理" class="headerlink" title="管理"></a>管理</h1><h2 id="Web-页面管理"><a href="#Web-页面管理" class="headerlink" title="Web 页面管理"></a>Web 页面管理</h2><p>如果在配置中开启了 http 服务，则可以直接访问 http 地址，输入账户密码后即可。<br><img src="https://i.loli.net/2020/01/16/c9oIUnOE4KTMJFf.png" alt></p><h2 id="通过-supervisorctl-管理"><a href="#通过-supervisorctl-管理" class="headerlink" title="通过 supervisorctl 管理"></a>通过 supervisorctl 管理</h2><p>比如要查看所有任务状态：<br><code>supervisorctl status all</code><br>或者重启所有服务：<br><code>supervisorctl restart all</code></p><p>更多命令可参见：<br><a href="http://supervisord.org/running.html#supervisorctl-command-line-options" target="_blank" rel="noopener">supervisorctl-command-line-options</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有时候我们希望有些进程能时刻运行，并能在出错后自动重启。除了用系统带的 systemd 之外，你还可以用supervisord。&lt;br&gt;这里简单介绍 supervisord 的安装和使用。&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://jiezhi.github.io/categories/Linux/"/>
    
    
      <category term="tools" scheme="http://jiezhi.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>sqoop 常见问题</title>
    <link href="http://jiezhi.github.io/2019/12/12/sqoop-issues/"/>
    <id>http://jiezhi.github.io/2019/12/12/sqoop-issues/</id>
    <published>2019-12-12T02:48:05.000Z</published>
    <updated>2019-12-16T07:33:48.903Z</updated>
    
    <content type="html"><![CDATA[<p>记录 Sqoop 使用过程中常见问题及解决方案。<br><a id="more"></a></p><h2 id="1-表名为数据库的保留字段"><a href="#1-表名为数据库的保留字段" class="headerlink" title="1.表名为数据库的保留字段"></a>1.表名为数据库的保留字段</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM order AS t WHERE 1=0</span><br><span class="line">ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error <span class="keyword">in</span> your SQL syntax; check the manual that corresponds to your MySQL server version <span class="keyword">for</span> the right syntax to use near <span class="string">'order AS t WHERE 1=0'</span> at line 1</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error <span class="keyword">in</span> your SQL syntax; check the manual that corresponds to your MySQL server version <span class="keyword">for</span> the right syntax to use near <span class="string">'order AS t WHERE 1=0'</span> at line 1</span><br></pre></td></tr></table></figure><p>Sqoop 在执行任务前，会执行类似的语句来判断源表是否有问题：<br><code>SELECT t.* FROM TABLE_NAME AS t WHERE 1=0</code><br>然而这里报的错表示这句语句有错误，无法执行。仔细一看原来业务库那边的表名为 <strong>order</strong>，显然这个订单表名使用了 MySQL 中的保留关键字了。在 MySQL 中，使用 <em><code></code></em> 将含关键字的名称引起来即可正常执行，如：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> t.* <span class="keyword">FROM</span> <span class="string">`order`</span> <span class="keyword">AS</span> t <span class="keyword">WHERE</span> <span class="number">1</span>=<span class="number">0</span>。</span><br></pre></td></tr></table></figure></p><p>如果在 Sqoop 命令中直接使用 <em><code>order</code></em> 却无法正常执行，<strong>order</strong> 会被 shell 当成独立的命令来执行，也会直接报错。（经过测试使用单引号<code>&#39;&#39;</code>、双引号<code>&quot;&quot;</code>及转义字符都无法执行 Sqoop 任务。)</p><p>Updated: 可以试下方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--table \`order\`</span><br></pre></td></tr></table></figure></p><p>但 sqoop 在找主键时似乎会出错，所以最好通过<code>--split-by</code>指定主键）</p><p>那可以考虑<br>1.用 <code>--query</code>  来解决</p><p>可参考<a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_free_form_query_imports" target="_blank" rel="noopener">Free-form Query Imports</a></p><p>2.将 sqoop 语句放到文件中，然后使用<code>sqoop --options-file</code>来执行对应的文件<br>可参考<a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_using_options_files_to_pass_arguments" target="_blank" rel="noopener">Using Options Files to Pass Arguments</a></p><h2 id="2-MySQL-中-Time-类型错误"><a href="#2-MySQL-中-Time-类型错误" class="headerlink" title="2.MySQL 中 Time 类型错误"></a>2.MySQL 中 Time 类型错误</h2><p>在数据库中，比如门店营业时间有的会填写<code>24:00:00</code>，但是在 sqoop 导入过程中会出现如下错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Error: java.io.IOException: SQLException <span class="keyword">in</span> nextKeyValue</span><br><span class="line">        at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:275)</span><br><span class="line">        at org.apache.hadoop.mapred.MapTask<span class="variable">$NewTrackingRecordReader</span>.nextKeyValue(MapTask.java:568)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)</span><br><span class="line">        at org.apache.hadoop.mapreduce.lib.map.WrappedMapper<span class="variable">$Context</span>.nextKeyValue(WrappedMapper.java:91)</span><br><span class="line">        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)</span><br><span class="line">        at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)</span><br><span class="line">        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)</span><br><span class="line">        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)</span><br><span class="line">        at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:174)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)</span><br><span class="line">        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)</span><br><span class="line">Caused by: java.sql.SQLException: java.sql.SQLException: Illegal hour value <span class="string">'24'</span> <span class="keyword">for</span> java.sql.Time <span class="built_in">type</span> <span class="keyword">in</span> value <span class="string">'24:00:00.</span></span><br><span class="line"><span class="string">        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1056)</span></span><br><span class="line"><span class="string">        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:957)</span></span><br><span class="line"><span class="string">        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:927)</span></span><br><span class="line"><span class="string">        at com.mysql.jdbc.ResultSet.getTimeFromString(ResultSet.java:6096)</span></span><br><span class="line"><span class="string">        at com.mysql.jdbc.ResultSet.getStringInternal(ResultSet.java:5819)</span></span><br><span class="line"><span class="string">        at com.mysql.jdbc.ResultSet.getString(ResultSet.java:5645)</span></span><br><span class="line"><span class="string">        at org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:68)</span></span><br><span class="line"><span class="string">        at partner_stores.readFields(partner_stores.java:505)</span></span><br><span class="line"><span class="string">        at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:242)</span></span><br><span class="line"><span class="string">        ... 12 more</span></span><br></pre></td></tr></table></figure><p>这是因为 sqoop 导入 MySQL 用的是 JDBC，然而 JDBC 的驱动中对 Time 类型最大支持到”23:59:59”，所以当 Time 类型数据出现超过”23:59:59”数据就会报这个错。</p><p>尝试过使用<code>--map-column-java</code>，但依然会报错<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--map-column-java</span><br><span class="line">endTime=String</span><br></pre></td></tr></table></figure></p><p>解决办法：<br>1.让业务人员修改数据库，要么把类型改成 Varchar 类型，要么把数据刷成最大”23:59:59”<br>2.自己动手, 使用<code>--query</code>，在 select 操作中进行类型转换：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">CAST</span>(startTime <span class="keyword">AS</span> <span class="built_in">CHAR</span>) startTime, <span class="keyword">CAST</span>(endTime <span class="keyword">AS</span> <span class="built_in">CHAR</span>) endTime</span><br><span class="line"><span class="keyword">FROM</span> shop.stores</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录 Sqoop 使用过程中常见问题及解决方案。&lt;br&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://jiezhi.github.io/categories/BigData/"/>
    
    
      <category term="Sqoop" scheme="http://jiezhi.github.io/tags/Sqoop/"/>
    
      <category term="HDFS" scheme="http://jiezhi.github.io/tags/HDFS/"/>
    
      <category term="Hive" scheme="http://jiezhi.github.io/tags/Hive/"/>
    
      <category term="MySQ" scheme="http://jiezhi.github.io/tags/MySQ/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop 增量更新</title>
    <link href="http://jiezhi.github.io/2019/09/05/sqoop-incremental/"/>
    <id>http://jiezhi.github.io/2019/09/05/sqoop-incremental/</id>
    <published>2019-09-05T02:18:58.000Z</published>
    <updated>2019-09-05T02:25:11.819Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>Sqoop 增量更新数据一般有两种方式：</p><h2 id="append-模式"><a href="#append-模式" class="headerlink" title="append 模式"></a>append 模式</h2><p>该模式适用于表中只有新数据添加的情况</p><h2 id="lastmodified-模式"><a href="#lastmodified-模式" class="headerlink" title="lastmodified 模式"></a>lastmodified 模式</h2><p>该模式适用于现有数据可能被修改的情况</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;Sqoop 增量更新数据一般有两种方式：&lt;/p&gt;
&lt;h2 id=&quot;append-模式&quot;&gt;&lt;a href=&quot;#append-模式&quot; class=&quot;headerlink&quot; title=&quot;append 模式&quot;&gt;&lt;/a&gt;append 模式&lt;/h2
      
    
    </summary>
    
      <category term="BigData" scheme="http://jiezhi.github.io/categories/BigData/"/>
    
    
      <category term="MySQL" scheme="http://jiezhi.github.io/tags/MySQL/"/>
    
      <category term="Sqoop" scheme="http://jiezhi.github.io/tags/Sqoop/"/>
    
      <category term="HDFS" scheme="http://jiezhi.github.io/tags/HDFS/"/>
    
      <category term="Hive" scheme="http://jiezhi.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>设置 idea 中的 Vim（同样适用于jetbrains 家其他产品）</title>
    <link href="http://jiezhi.github.io/2019/08/30/set-vim-in-idea/"/>
    <id>http://jiezhi.github.io/2019/08/30/set-vim-in-idea/</id>
    <published>2019-08-30T01:33:38.000Z</published>
    <updated>2019-08-30T01:55:46.470Z</updated>
    
    <content type="html"><![CDATA[<p>之前每次启动 idea 或 pycharm 后都要单独设置下才开始工作，那么怎么才能像设置 vimrc 一样设置 idea 中的 vim 呢？<br><a id="more"></a></p><p>可能很多人.vimrc 文件中都有这个设置：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set relativenumber</span><br><span class="line">set number</span><br></pre></td></tr></table></figure></p><img src="/2019/08/30/set-vim-in-idea/vim-rnu.png"><p>这个设置使得上下跳转（j/k）再也不用去计算行数了.</p><p>idea 中的 vim 设置也像.vimrc一样，只不过设置在<code>.ideavimrc</code>文件中。</p><p>所以只要在在~/.ideavimrc 文件中加上以下配置即可（有其他vim设置也可以加入到这里）:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set relativenumber</span><br><span class="line">set number</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前每次启动 idea 或 pycharm 后都要单独设置下才开始工作，那么怎么才能像设置 vimrc 一样设置 idea 中的 vim 呢？&lt;br&gt;
    
    </summary>
    
      <category term="Tools" scheme="http://jiezhi.github.io/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>通过 ssh 远程代理，访问远程资源</title>
    <link href="http://jiezhi.github.io/2019/03/29/tunnel-server-socket-over-ssh/"/>
    <id>http://jiezhi.github.io/2019/03/29/tunnel-server-socket-over-ssh/</id>
    <published>2019-03-29T02:10:26.000Z</published>
    <updated>2019-03-29T02:49:25.530Z</updated>
    
    <content type="html"><![CDATA[<p>平时我们使用SSH大多用来，登录远程服务器来进行远程操作。</p><p>但是 SSH 强大的功能远远不止登录远程服务器，比如说我在服务器上部署了一个服务，但是端口号并没有对外开放怎么办？又或者生产环境的数据库本地无法连接，而只能通过线上环境连接怎么办？<br><a id="more"></a></p><h1 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h1><h2 id="SSH-访问远程页面"><a href="#SSH-访问远程页面" class="headerlink" title="SSH 访问远程页面"></a>SSH 访问远程页面</h2><p><em>在 Shadowsocks 之前，很多人是使用 SSH 来跨过长城的。</em><br>现在遇到的问题，生产环境部署了 Flink，虽说 Flink 提供了 REST Api 来查看信息的，但是查看页面还是更方便点。但是又没办法连接生产环境怎么办？<br>其实可以通过 SSH 建立本地和服务器的通道，进而可以访问远程服务器的资源。<br>比如，flink 页面默认端口号为8081，在终端里执行这个命令后，即可通过访问本地的 localhost:8081 访问服务器上的 flink 页面。<br><code>ssh -L 8081:localhost:8081 user@flinkserver</code></p><h2 id="SSH-代理访问数据库"><a href="#SSH-代理访问数据库" class="headerlink" title="SSH 代理访问数据库"></a>SSH 代理访问数据库</h2><p>其实道理和上面的一样，一般生产环境数据库我们是没法直接连接的。不过只要有可以访问该数据库的其他服务器 SSH 配置就行了，同一个原理，但实现方式就很多了。</p><ol><li><p>直接登录服务器，通过 mysql 命令来访问。简单粗暴，但比较繁琐。</p></li><li><p>使用支持 SSH tunnel 的客户端来连接数据库，比如 <a href="https://www.jetbrains.com/datagrip/" target="_blank" rel="noopener">DataGrip</a></p><img src="/2019/03/29/tunnel-server-socket-over-ssh/datagrip_ssh.png"></li><li><p>同样，使用 ssh 命令来建立通道，本地就可以访问了。<br>比如数据库地址为：192.168.100.100:3306，远程服务器为：<a href="mailto:root@192.168.10.200" target="_blank" rel="noopener">root@192.168.10.200</a>，使用本地3307端口来映射（其他端口也行，我因为本地3306被用了，所以用3307了）<br><code>ssh -L 3307:192.168.100.100:3306 root@192.168.10.200</code></p></li></ol><p>执行过后，只需要连接 <code>localhost:3307</code> 就可以访问数据库了。</p><hr><p>SSH 还有很多强大的功能，下次有空再写一下配置的优化。</p><hr><p>Reference:<br><a href="https://askubuntu.com/questions/112177/how-do-i-tunnel-and-browse-the-server-webpage-on-my-laptop" target="_blank" rel="noopener">https://askubuntu.com/questions/112177/how-do-i-tunnel-and-browse-the-server-webpage-on-my-laptop</a></p><p><a href="https://unix.stackexchange.com/questions/14160/ssh-tunneling-error-channel-1-open-failed-administratively-prohibited-open" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/14160/ssh-tunneling-error-channel-1-open-failed-administratively-prohibited-open</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时我们使用SSH大多用来，登录远程服务器来进行远程操作。&lt;/p&gt;
&lt;p&gt;但是 SSH 强大的功能远远不止登录远程服务器，比如说我在服务器上部署了一个服务，但是端口号并没有对外开放怎么办？又或者生产环境的数据库本地无法连接，而只能通过线上环境连接怎么办？&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://jiezhi.github.io/categories/Linux/"/>
    
    
      <category term="ssh" scheme="http://jiezhi.github.io/tags/ssh/"/>
    
  </entry>
  
  <entry>
    <title>Flink 分布式运行时环境（Distributed Runtime Environment）</title>
    <link href="http://jiezhi.github.io/2019/03/08/flink-distributed-runtime-environment/"/>
    <id>http://jiezhi.github.io/2019/03/08/flink-distributed-runtime-environment/</id>
    <published>2019-03-08T06:26:26.000Z</published>
    <updated>2019-03-08T09:41:05.222Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章介绍 Flink 的分布式运行时环境相关的概念。<br><a id="more"></a></p><p>本文译自<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/concepts/runtime.html" target="_blank" rel="noopener">Flink官网</a></p><h2 id="任务和操作链"><a href="#任务和操作链" class="headerlink" title="任务和操作链"></a>任务和操作链</h2><p>对于分布式的执行，Flink将操作子任务<em>链接(chain)</em>到<em>任务(task)</em>中。每个任务由一个线程执行。将操作链接到任务是一项有用的优化：它可以减少线程到线程切换和缓冲的开销，并在降低延迟的同时提高整体吞吐量。链接行为可以配置;有关详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/#task-chaining-and-resource-groups" target="_blank" rel="noopener">链接文档</a>。</p><p>下图中的示例数据流由五个子任务执行，因此具有五个并行线程。</p><img src="/2019/03/08/flink-distributed-runtime-environment/tasks_chains.svg"><p>操作链接导任务</p><h2 id="作业管理器，任务管理器和客户端"><a href="#作业管理器，任务管理器和客户端" class="headerlink" title="作业管理器，任务管理器和客户端"></a>作业管理器，任务管理器和客户端</h2><p>Flink运行时包含两种类型的进程：</p><ul><li><p>作业管理器（<strong>JobManagers，</strong>也称为<em>master</em>）协调分布式执行。他们安排任务，协调检查点，协调故障恢复等。</p><p>  至少有一个Job Manager。设置高可用（HA, High-Availability）的话将具有多个JobManagers，其中一个始终是<em>领导者(leader)</em>，其它的处于<em>待机状态(standby)</em>。</p></li><li><p>任务管理器（<strong>TaskManagers，</strong>也称为<em>worker</em>）执行数据流的任务（或更具体地说，子任务），并缓冲和交换数据流。</p><p>  必须始终至少有一个TaskManager。</p></li></ul><p>JobManagers和TaskManagers可以通过多种方式启动：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/cluster_setup.html" target="_blank" rel="noopener">独立集群</a>直接在机器里启动，或在容器中，或由<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">YARN</a>或<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/mesos.html" target="_blank" rel="noopener">Mesos</a>等资源框架来管理。 TaskManagers连接到JobManagers，通知自己可用，然后被分配任务。</p><p><strong>客户端</strong>不是运行时和程序执行的一部分，而是被用于准备数据流并将数据流发送到JobManager。之后，客户端可以断开连接或保持连接以接收进度报告。客户端要么作为触发执行的Java / Scala程序的一部分运行，要么是在命令行进程中运行 ./bin/flink run ….</p><img src="/2019/03/08/flink-distributed-runtime-environment/processes.svg"><p>执行Flink数据流所涉及的过程</p><h2 id="任务槽和资源"><a href="#任务槽和资源" class="headerlink" title="任务槽和资源"></a>任务槽和资源</h2><p>每个worker（TaskManager）都是一个<em>JVM进程</em>，可以在不同的线程中执行一个或多个子任务。为了控制worker接受的任务数量，worker中有<strong>任务槽（task slots）</strong>（至少一个）。</p><p>每个<em>任务槽</em>代表TaskManager的固定资源子集。例如，具有三个任务槽的TaskManager会将其1/3的托管内存分配到每个插槽。切换资源意味着子任务不会与来自其他作业的子任务竞争托管内存，而是竞争具有一定量的保留托管内存。请注意，这里没有CPU隔离;当前任务槽只分离任务的托管内存。</p><p>通过调整任务槽的数量，用户可以定义子任务之间如何相互隔离。每个TaskManager有一个插槽意味着每个任务组在一个单独的JVM中运行（例如，可以在一个单独的容器中启动）。拥有多个插槽意味着更多子任务共享同一个JVM。同一JVM中的任务共享TCP连接（通过多路复用）和心跳消息。它们还可以共享数据集和数据结构，从而减少每任务开销。</p><img src="/2019/03/08/flink-distributed-runtime-environment/tasks_slots.svg"><p>具有任务槽和任务的TaskManager</p><p>默认情况下，Flink允许子任务共享插槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果就是，一个槽可以保存作业的整个管道。允许此插槽共享有两个主要好处：</p><ul><li>Flink集群需要的任务槽数量与作业中使用的最高并行度的任务槽一样多。无需计算程序总共包含多少任务（具有不同的并行性）。</li><li><p>更容易获得更好的资源利用率。没有插槽共享，非密集的<em>source/ map()</em>子任务将占用与资源密集型<em>窗口</em>子任务一样多的资源。通过插槽共享，将示例中的基本并行性从2增加到6可以充分利用时隙资源，同时确保繁重的子任务在TaskManagers之间公平分配。</p>  <img src="/2019/03/08/flink-distributed-runtime-environment/slot_sharing.svg"><p>  具有共享任务槽的TaskManagers</p></li></ul><p>API还包括<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/#task-chaining-and-resource-groups" target="_blank" rel="noopener">资源组</a>机制，可用于防止意外的插槽共享。</p><p>根据经验，一个很好的默认任务槽数就是CPU核心数。使用超线程，每个插槽然后需要2个或更多硬件线程。</p><h2 id="State-Backends-状态后端"><a href="#State-Backends-状态后端" class="headerlink" title="State Backends(状态后端?)"></a>State Backends(状态后端?)</h2><p>存储键/值索引的确切数据结构取决于所选的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/state_backends.html" target="_blank" rel="noopener">state backend</a>。一个state backend将数据存储在内存中的哈希映射中，另一个state backend使用<a href="http://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>作为键/值存储。除了定义保存状态的数据结构之外，state backend还实现了获取键/值状态的时间点快照，并将该快照存储为检查点的一部分的逻辑。</p><img src="/2019/03/08/flink-distributed-runtime-environment/checkpoints.svg"><p>检查点和快照</p><h2 id="保存点（Savepoints）"><a href="#保存点（Savepoints）" class="headerlink" title="保存点（Savepoints）"></a>保存点（Savepoints）</h2><p>用Data Stream API编写的程序可以从<strong>保存点</strong>中恢复执行。保存点允许更新程序和Flink群集，而不会丢失任何状态。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/savepoints.html" target="_blank" rel="noopener">保存点</a>是<strong>手动触发的检查点</strong>，它可以生成一个程序的快照并把它写到state backend。 他们依靠常规的检查点机制。 在执行期间，程序会定期在工作节点上创建快照并生成检查点。 对于程序恢复，仅需要最后完成的检查点，并且一旦完成新检查点，就可以安全地丢弃旧检查点。</p><p>保存点与这些定期检查点类似，不同之处在于它们由<strong>用户触发</strong>，并且在较新的检查点完成时<strong>不会自动过期</strong>。 可以从<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/cli.html#savepoints" target="_blank" rel="noopener">命令行</a>创建保存点，也可以通过<strong><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/monitoring/rest_api.html#cancel-job-with-savepoint" target="_blank" rel="noopener">REST API</a></strong>取消作业。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章介绍 Flink 的分布式运行时环境相关的概念。&lt;br&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://jiezhi.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="http://jiezhi.github.io/tags/Flink/"/>
    
      <category term="Translate" scheme="http://jiezhi.github.io/tags/Translate/"/>
    
  </entry>
  
  <entry>
    <title>解决 macos systemuiserver 无响应的问题</title>
    <link href="http://jiezhi.github.io/2019/03/07/macos-systemuiserver-not-responding/"/>
    <id>http://jiezhi.github.io/2019/03/07/macos-systemuiserver-not-responding/</id>
    <published>2019-03-07T01:28:50.000Z</published>
    <updated>2019-03-07T01:41:05.651Z</updated>
    
    <content type="html"><![CDATA[<p>把 MBP 升级到最新系统（10.14.3）后，发现休眠后再打开电脑，点击无线网或切换wifi 的时候，系统开始转菊花，系统设置也无响应了，去Activity Monitor 里会发现 <strong>systemuiserver not responding</strong>，貌似除了重启别无他法。</p><p>当然除了重启，你只需要在命令行里执行一个命令就可以了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">kill</span> -9 `ps aux | grep -v grep | grep /usr/libexec/airportd | awk <span class="string">'&#123;print $2&#125;'</span>`</span><br></pre></td></tr></table></figure><a id="more"></a><p>这行命令的意思是强行杀死airportd的进程，然后会导致相关进程重启。虽说没有从根本上解决问题，但比重启系统好多了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;把 MBP 升级到最新系统（10.14.3）后，发现休眠后再打开电脑，点击无线网或切换wifi 的时候，系统开始转菊花，系统设置也无响应了，去Activity Monitor 里会发现 &lt;strong&gt;systemuiserver not responding&lt;/strong&gt;，貌似除了重启别无他法。&lt;/p&gt;
&lt;p&gt;当然除了重启，你只需要在命令行里执行一个命令就可以了。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo &lt;span class=&quot;built_in&quot;&gt;kill&lt;/span&gt; -9 `ps aux | grep -v grep | grep /usr/libexec/airportd | awk &lt;span class=&quot;string&quot;&gt;&#39;&amp;#123;print $2&amp;#125;&#39;&lt;/span&gt;`&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="MacOS" scheme="http://jiezhi.github.io/categories/MacOS/"/>
    
    
      <category term="Macos" scheme="http://jiezhi.github.io/tags/Macos/"/>
    
      <category term="Bug" scheme="http://jiezhi.github.io/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title>Flink编程模型</title>
    <link href="http://jiezhi.github.io/2019/03/04/flink-concepts-programming-model/"/>
    <id>http://jiezhi.github.io/2019/03/04/flink-concepts-programming-model/</id>
    <published>2019-03-04T02:16:57.000Z</published>
    <updated>2019-03-08T09:40:58.301Z</updated>
    
    <content type="html"><![CDATA[<p>Flink 的编程模型为数据流编程模型（Dataflow Programming Model），这里介绍编程模型里面的几个概念。<br><a id="more"></a></p><p>本文译自Flink官网：<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/concepts/programming-model.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Dataflow Programming Model</a></p><p>数据流编程模型（Dataflow Programming Model）</p><h2 id="抽象层次（Levels-of-Abstraction）"><a href="#抽象层次（Levels-of-Abstraction）" class="headerlink" title="抽象层次（Levels of Abstraction）"></a>抽象层次（Levels of Abstraction）</h2><p><a href="https://www.notion.so/1bd6bd9f543540b7be20ecdd8848cc4e#b8c5801ae50045e0bf336e6ae77542be" target="_blank" rel="noopener"></a><br><img src="/2019/03/04/flink-concepts-programming-model/levels_of_abstraction.svg"><br><em>Flink 提供几种不同层次的抽象来开发 <strong>流/批（streaming/batch）</strong>程序</em></p><ul><li>最低级的抽象仅提供<strong>状态流（stateful streaming）</strong>，它通过 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/process_function.html" target="_blank" rel="noopener">Process Function</a> （处理函数）内嵌在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html" target="_blank" rel="noopener">DataStream API</a> 中。它容许用户自由地处理来自一个或多个流的事件，并且使用一致的容错状态。此外，用户也可以给事件时间和处理时间注册回调，使得程序可以实现复杂的计算。</li><li><p>实践中，多数的应用程序不需要使用上述的低级的抽象，仅需要使用<strong>核心接口（Core API）</strong>来编码，比如 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html" target="_blank" rel="noopener">DataStream API</a> (数据流接口，有界/无界流) 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/index.html" target="_blank" rel="noopener">DataSet API</a> （数据集接口，有界数据集）。这些流畅的接口为数据处理提供了通用构建流程，诸如用户指定的转换（transformation）、连接（join）、聚合（aggregation）、窗口（window）、状态（state）等不同形式。这些接口处理的数据类型在不同的编程语言中以类（class）的形式呈现。</p><p>  低层次的<em>处理函数（Process Function）</em>与<em>数据流接口（</em><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html" target="_blank" rel="noopener">DataStream API</a><em>）</em>的交互，使得某些特定的操作可以抽象为更低的层次成为可能。<em>数据集接口（DataSet API）</em>在有界的数据集上提供额外的原始操作，例如循环和迭代（loops/iterations）。</p></li><li><p><strong>表接口（Table API）</strong>使以表为中心的声明性 DSL，可以动态地改变表（当展示流的时候）。Table API遵循（扩展）关系型模型：表附加了一个模式(schema)（类似于关系型数据库中的表），此API提供了可比较的操作，例如select，project，join，group-by，aggregate等。Table API程序以声明方式定义应该执行的逻辑操作，而不是准确地指定操作代码。 尽管Table API可以通过各种类型的用户定义函数进行扩展，但它的表现力不如Core API，但使用起来更简洁（编写的代码更少）。 此外，Table API程序还会通过优化程序，在执行之前应用优化规则。</p><p>  可以在表和DataStream/ DataSet之间无缝转换，允许在程序中混合Table API以及DataStream和DataSet API。</p></li><li><p>Flink提供的最高级抽象是<strong>SQL</strong>。 这种抽象在语义和表达方面类似于Table API，但是将程序表示为SQL查询表达式。 SQL抽象与Table API紧密交互，SQL查询可以在Table API中定义的表上执行。</p></li></ul><h2 id="程序和数据流（Programs-and-Dataflows）"><a href="#程序和数据流（Programs-and-Dataflows）" class="headerlink" title="程序和数据流（Programs and Dataflows）"></a>程序和数据流（Programs and Dataflows）</h2><p>Flink程序的基本构建块是<strong>流（streams）</strong>和<strong>转换（transformations）</strong>。 （请注意，Flink的DataSet API中使用的DataSet也是内部流，稍后会详细介绍。）从概念上讲，<em>流</em>是（可能永无止境的）数据记录流，而<em>转换</em>是将一个或多个流作为输入，并产生一个或多个输出流的操作。 </p><p>执行时，Flink程序映射到<strong>流数据流（streaming dataflows）</strong>，由<strong>流(streams)</strong>和转换<strong>运算符(operators)</strong>组成。 每个数据流都以一个或多个<strong>源(sources)</strong>开头，并以一个或多个<strong>接收器(sinks)</strong>结束。 数据流类似于任意<strong>有向无环图（DAGs, Directed acyclic graphs）</strong>。 尽管通过<em>迭代结构</em>允许特殊形式的循环，但为了简单起见，我们将在大多数情况下对其进行<del>掩饰</del>简化。</p><img src="/2019/03/04/flink-concepts-programming-model/program_dataflow.svg"><p>通常，程序中的转换与数据流中的运算符之间存在一对一的对应关系。 但是，有时一个转换可能包含多个转换运算符。</p><p>源（soruces）和接收器（sinks）被记录在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/index.html" target="_blank" rel="noopener">流连接器</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/connectors.html" target="_blank" rel="noopener">批处理连接器</a>文档中。 转换（transformation）被记录在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/index.html" target="_blank" rel="noopener">DataStream运算符</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/dataset_transformations.html" target="_blank" rel="noopener">DataSet转换</a>中。</p><h2 id="并行数据流"><a href="#并行数据流" class="headerlink" title="并行数据流"></a>并行数据流</h2><p>Flink中的程序本质上是并行（parallel）和分布式的（distributed）。 在执行期间，<em>流</em>具有一个或多个<em>流分区（stream partitions）</em>，并且每个<em>运算符</em>具有一个或多个<em>运算子任务(operator subtasks)</em>。 运算子任务彼此独立，并且可以在不同的线程中执行，也可能是在不同的机器或容器上执行。</p><p>运算子任务的数量就是某个特定运算符的<strong>并行度（parallelism）</strong>。 流的并行度始终是其生成的运算符的并行度。 同一程序的不同运算符可能具有不同的并行级别。</p><img src="/2019/03/04/flink-concepts-programming-model/parallel_dataflow.svg"><p>流可以以<em>一对一（或转发）</em>的模式或以<em>重新分发</em>的模式在两个运算符之间传输数据：</p><ul><li><strong>一对一（One-to-one）</strong>流（例如，在上图中的<em>Source</em>和<em>map()</em>运算符之间）保留元素的分区和排序。这意味着<em>map()</em>运算符的subtask[1]看到的元素与<em>Source</em>运算符的subtask[1]生成的元素顺序相同。</li><li><strong>重新分发（Redistributing）</strong>流（在上面的<em>map()</em>和<em>keyBy/window</em>之间，以及<em>keyBy/window</em>和<em>Sink</em>之间）重新分配流的分区。每个<em>运算子任务</em>将数据发送到不同的目标子任务，具体取决于所选的转换。示例是<em>keyBy()</em>（通过散列键重新分区），<em>broadcast()</em>或<em>rebalance()</em>（随机重新分区）。在<em>重新分发</em>的交换中，元素之间的排序仅保留在每对发送和接收子任务中（例如，<em>map()</em>的subtask[1]和keyBy/window的subtask[2]）。因此，在此示例中，保留了每个键的排序，但并行度确实带来了不同键的聚合结果到达sink的顺序的不确定性。</li></ul><p>有关配置和控制并行性的详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/parallel.html" target="_blank" rel="noopener">并行执行</a>的文档。</p><h2 id="窗口（Windows）"><a href="#窗口（Windows）" class="headerlink" title="窗口（Windows）"></a>窗口（Windows）</h2><p>聚合事件（如，counts，sums）在流上的工作方式与批处理方式不同。 例如，不可能计算流中的所有元素，因为流通常是无限的（无界）。 相反，流上的聚合（counts，sums等）由<strong>窗口(windows)</strong>限定，例如“在最后5分钟内计数”或“最后100个元素的总和”。</p><p>Windows可以是时间驱动的（例如：每30秒）或数据驱动（例如：每100个元素）。 人们通常区分不同类型的窗口，例如<strong><em>翻滚窗口(tumbling windows)</em></strong>（没有重叠），<strong><em>滑动窗口(sliding windows)</em></strong>（具有重叠）和<strong><em>会话窗口(session windows)</em></strong>（由不活动间隙打断）。</p><img src="/2019/03/04/flink-concepts-programming-model/windows.svg"><p>可以在<a href="https://flink.apache.org/news/2015/12/04/Introducing-windows.html" target="_blank" rel="noopener">此博客</a>文章中找到更多窗口示例。 更多详细信息可参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/windows.html" target="_blank" rel="noopener">窗口文档</a>。</p><h2 id="时间（Time）"><a href="#时间（Time）" class="headerlink" title="时间（Time）"></a>时间（Time）</h2><p>当在流程序中引用时间（例如定义窗口）时，可以参考不同的时间概念：</p><ul><li><strong>事件时间（Event Time）</strong>是创建事件的时间。 它通常由事件中的时间戳描述，例如由生产传感器或生产服务生成。 Flink通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/event_timestamps_watermarks.html" target="_blank" rel="noopener">时间戳分配器</a>（timestamp assigners）访问事件时间戳。</li><li><strong>接收时间(Ingestion Time)</strong>是事件在源操作符处进入Flink数据流的时间。</li><li><strong>处理时间（Processing Time）</strong>是每个操作符执行基于时间的操作时的本地时间。</li></ul><img src="/2019/03/04/flink-concepts-programming-model/event_ingestion_processing_time.svg"><p>事件时间，接收时间和处理时间</p><p>有关如何处理时间的更多详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/event_time.html" target="_blank" rel="noopener">事件时间文档</a>。</p><h2 id="状态运算（Stateful-Operations）"><a href="#状态运算（Stateful-Operations）" class="headerlink" title="状态运算（Stateful Operations）"></a>状态运算（Stateful Operations）</h2><p>虽然数据流中的许多运算只是一次查看一个单独的事件（例如事件解析器），但某些运算会记住多个事件（例如窗口运算符）的信息。这些操作称为<strong>stateful</strong>。</p><p>状态运算的状态可以被认为是由内嵌的键/值存储来维护。状态和状态运算符读取的流被严格地分区和分发。因此，只有在<em>keyBy()</em>函数之后才能在keyed stream上访问键/值状态，并且限制为与当前事件的键相关联的值。对齐流和状态的键可确保所有状态更新都是本地操作，从而保证一致性而无需事务开销。对齐操作还允许Flink重新分配状态并透明地调整流分区。</p><img src="/2019/03/04/flink-concepts-programming-model/state_partitioning.svg"><p>状态和分区</p><p>有关更多信息，请参阅有关<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/index.html" target="_blank" rel="noopener">状态</a>的文档。</p><h2 id="容错检查点-Checkpoints-for-Fault-Tolerance"><a href="#容错检查点-Checkpoints-for-Fault-Tolerance" class="headerlink" title="容错检查点(Checkpoints for Fault Tolerance)"></a>容错检查点(Checkpoints for Fault Tolerance)</h2><p>Flink使用<strong>stream replay</strong>和<strong>检查点(checkpointng)</strong>的组合来实现容错。检查点与每个输入流中的特定点以及每个运算符的对应状态相关。通过恢复运算符的状态并从检查点重新执行（replay）事件，可以从检查点恢复流数据流并保持一致性（exactly-once processing semantics）。</p><p>检查点间隔是执行期间的容错和恢复时间（需要重放的事件的数量）之间的折衷方法。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html" target="_blank" rel="noopener">容错的内部机制</a>中的描述提供了有关Flink如何管理检查点和相关主题的更多信息。有关启用和配置检查点的详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/checkpointing.html" target="_blank" rel="noopener">检查点API文档</a>。</p><h2 id="批处理流"><a href="#批处理流" class="headerlink" title="批处理流"></a>批处理流</h2><p>Flink执行<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/index.html" target="_blank" rel="noopener">批处理程序</a>作为流程序的一种特殊情况，即流是有界的（有限数量的元素）。 <em>DataSet</em>在内部被视为数据流。因此，上述概念以相同的方式应用于批处理程序，并且它们适用于流程序，除了少数例外：</p><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/fault_tolerance.html" target="_blank" rel="noopener">批处理程序的容错</a>不使用检查点。通过完全重新执行流来进行恢复，因为输入是有限的。这会使资源更多地用于恢复，且使得常规处理资源消耗更少，因为它避免了检查点。</li><li>DataSet API中的有状态操作（stateful operations）使用简化的内存/核外(in-memory/out-of-core)数据结构，而不是键/值索引。</li><li>DataSet API引入了特殊的同步（ superstep-based）迭代，这些迭代只能在有界流上进行。有关详细信息，请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/iterations.html" target="_blank" rel="noopener">迭代文档</a>。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Flink 的编程模型为数据流编程模型（Dataflow Programming Model），这里介绍编程模型里面的几个概念。&lt;br&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://jiezhi.github.io/categories/Flink/"/>
    
    
      <category term="Apache" scheme="http://jiezhi.github.io/tags/Apache/"/>
    
      <category term="Flink" scheme="http://jiezhi.github.io/tags/Flink/"/>
    
      <category term="Translate" scheme="http://jiezhi.github.io/tags/Translate/"/>
    
      <category term="Big Data" scheme="http://jiezhi.github.io/tags/Big-Data/"/>
    
  </entry>
  
  <entry>
    <title>分享学习大数据相关笔记，并邀请您加入</title>
    <link href="http://jiezhi.github.io/2019/02/26/join-bigdata-group/"/>
    <id>http://jiezhi.github.io/2019/02/26/join-bigdata-group/</id>
    <published>2019-02-26T06:25:51.000Z</published>
    <updated>2019-02-26T06:41:56.742Z</updated>
    
    <content type="html"><![CDATA[<p>用 Notion 有一段时间了，也积累了不少技术知识点和读书笔记。这里创建了『大数据』相关的<br> WORKSPACE，希望有志同道合的朋友一起加入，互相学习，共同进步。</p><a id="more"></a><h2 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h2><p><a href="http://www.notion.so/data365" target="_blank" rel="noopener">www.notion.so/data365</a> </p><p>项目里目前主要分为两块：</p><ul><li><p><a href="https://www.notion.so/data365/Inbox-ea89df07495547ceb210676e28eca371" target="_blank" rel="noopener">Inbox</a></p><p>  项目成员可以添加任何大数据相关的信息（包括但不局限于架构、调优、博客、笔记等）</p></li><li><p><a href="https://www.notion.so/data365/Big-Data-7a897ed23bad42da89075fd5b7ed2407" target="_blank" rel="noopener">Big Data</a></p><p>  经过分类编辑后的内容，目标是保证可读，并且有用</p></li></ul><h2 id="账号注册"><a href="#账号注册" class="headerlink" title="账号注册"></a>账号注册</h2><p>首先你需要有 Notion 的账号，如果没有可以先<a href="https://www.notion.so/?r=507f808e5cb544809dfe98d930ee2cae" target="_blank" rel="noopener">注册</a> （有aff）</p><h2 id="Notion-入门"><a href="#Notion-入门" class="headerlink" title="Notion 入门"></a>Notion 入门</h2><p><a href="https://www.notion.so/e040febf70a94950b8620e6f00005004" target="_blank" rel="noopener">Guides &amp; FAQs</a></p><p>强烈建议阅读官方入门文档。</p><h2 id="加入-Workspace"><a href="#加入-Workspace" class="headerlink" title="加入 Workspace"></a>加入 Workspace</h2><p>欢迎各位对大数据感兴趣的朋友加入，加入方式：</p><ol><li>可以<a href="mailto:Jiezhi007+notion@gmail.com" target="_blank" rel="noopener">发邮件</a>给我</li><li><a href="https://telegram.me/Jiezhi" target="_blank" rel="noopener">电报我</a></li><li>加入<a href="https://join.slack.com/t/bigdata365/shared_invite/enQtNTU5NjI0MDY0MjEyLWQ0MmVkNDZiOGU4Mjk0NjVhNTU3YjhmMmRlYTQxZmRhOTZmMzczNTM0YmUzNmE3YTNjNGQ2NmEwMDBhYmY5ODk" target="_blank" rel="noopener">Slack 群组</a>，(通过 slack 可以及时得到项目相关的通知)：</li><li><a href="https://t.me/joinchat/BbrpxRVra-X8U5VmM92L3Q" target="_blank" rel="noopener">电报群</a>（没法私信我的，可以先加群，然后我私信你） </li></ol><h2 id="权限说明"><a href="#权限说明" class="headerlink" title="权限说明"></a>权限说明</h2><p>Notion支持几种不同的权限，分别如下：</p><img src="/2019/02/26/join-bigdata-group/1.png"><ol><li>默认群组里成员在 <a href="https://www.notion.so/data365/Inbox-ea89df07495547ceb210676e28eca371" target="_blank" rel="noopener">Inbox</a> 里拥有权限为『Full Access』</li><li>默认群组在 <a href="https://www.notion.so/7a897ed2-3bad-42da-8907-5fd5b7ed2407" target="_blank" rel="noopener">Big Data</a> 里的权限为『Can Comment』，你可以阅读和评论但不能编辑。</li></ol><hr><blockquote><p>如有改动，可查看<a href="https://www.notion.so/data365/Readme-1717a7ef97384eacaf3aa040d53dd1df" target="_blank" rel="noopener">最新信息</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用 Notion 有一段时间了，也积累了不少技术知识点和读书笔记。这里创建了『大数据』相关的&lt;br&gt; WORKSPACE，希望有志同道合的朋友一起加入，互相学习，共同进步。&lt;/p&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://jiezhi.github.io/categories/BigData/"/>
    
    
      <category term="Note" scheme="http://jiezhi.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>CAP定理与BASE理论</title>
    <link href="http://jiezhi.github.io/2019/02/25/cap-with-base/"/>
    <id>http://jiezhi.github.io/2019/02/25/cap-with-base/</id>
    <published>2019-02-25T02:29:25.000Z</published>
    <updated>2019-02-26T06:24:16.621Z</updated>
    
    <content type="html"><![CDATA[<p>CAP: 是指 Consistency(一致性), Availability(可用性) 和 Partition Tolerance(可用性).<br>BASE: <strong>B</strong>asically <strong>A</strong>vailable（基本可用）, <strong>S</strong>oft state（软状态）和 <strong>E</strong>ventually consistent（最终一致性）<br><a id="more"></a></p><h2 id="CAP-定理"><a href="#CAP-定理" class="headerlink" title="CAP 定理"></a>CAP 定理</h2><img src="/2019/02/25/cap-with-base/cap.png" title="cap"><ul><li><p><strong>Consistency</strong> <strong>一致性</strong>（节点或系统中所有可用数据）</p><p>  在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一直的状态。</p></li><li><p><strong>Availability</strong> <strong>可用性</strong>（每个请求会得到一个回应）</p><p>  可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是”有限时间内”和”返回结果”。</p></li><li><p><strong>Partition Tolerance 分区容错性</strong>（系统运作将不管可用性、分区、数据或通信的丢失）</p><p>  分区容错性约束了一个分布式系统具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。</p></li></ul><h2 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h2><ul><li><p><strong>B</strong>asically <strong>A</strong>vailable（基本可用）</p><p>  分布式系统在出现不可预知故障的时候，允许损失部分可用性</p></li><li><p><strong>S</strong>oft state（软状态）</p><p>  软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据听不的过程存在延时。</p></li><li><p><strong>E</strong>ventually consistent（最终一致性）</p><p>  最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性</p></li></ul><p>CAP 与 BASE 关系为：</p><blockquote><p>在分布式的数据系统中，你能保证下面三个要求中的两个：一致性，可用性，以及分区容错性。在此模型上构建的系统将称作 BASE(基本上可用软状态最终一致)架构，不满足 ACID 性质。</p></blockquote><hr><h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><ul><li><p>Atomicity 原子性</p><p>  一个事务必须被视为一个不可分割的最小工作单元。</p></li><li><p>Consistency 一致性</p><p>  数据库总是从一个一致性的状态转换到另外一个一致性的状态。</p></li><li><p>Isolation 隔离性</p><p>  通常来说，一个事务所做的修改在最终提交之前，对其他事物是不可见的。</p></li><li><p>Durability 持久性</p><p>  一旦事务提交，则其所做的修改就会永久保存到数据库中。</p></li></ul><hr><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><ul><li><a href="https://hbase.apache.org/" target="_blank" rel="noopener">HBase</a>、<a href="http://www.hypertable.org/" target="_blank" rel="noopener">HyperTable</a> 和 <a href="https://www.wikiwand.com/en/Bigtable" target="_blank" rel="noopener">BigTable</a>，满足 CP 性质。</li><li><a href="http://cassandra.apache.org/" target="_blank" rel="noopener">Cassandra</a>、<a href="https://aws.amazon.com/dynamodb/" target="_blank" rel="noopener">Dynamo</a> 和 <a href="https://www.project-voldemort.com/voldemort/" target="_blank" rel="noopener">Voldemort</a>，满足 AP性质。</li></ul><hr><p>Reference：</p><p>《高性能MySQL》</p><p>《大数据与数据仓库》</p><p><a href="https://www.cnblogs.com/duanxz/p/5229352.html" target="_blank" rel="noopener">CAP原则(CAP定理)、BASE理论 - duanxz - 博客园</a></p><p><a href="https://my.oschina.net/u/1175305/blog/1998062" target="_blank" rel="noopener">CAP和BASE理论的关系 - leooooo的个人空间 - 开源中国</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CAP: 是指 Consistency(一致性), Availability(可用性) 和 Partition Tolerance(可用性).&lt;br&gt;BASE: &lt;strong&gt;B&lt;/strong&gt;asically &lt;strong&gt;A&lt;/strong&gt;vailable（基本可用）, &lt;strong&gt;S&lt;/strong&gt;oft state（软状态）和 &lt;strong&gt;E&lt;/strong&gt;ventually consistent（最终一致性）&lt;br&gt;
    
    </summary>
    
      <category term="NoSQL" scheme="http://jiezhi.github.io/categories/NoSQL/"/>
    
    
      <category term="NoSQL" scheme="http://jiezhi.github.io/tags/NoSQL/"/>
    
      <category term="CAP" scheme="http://jiezhi.github.io/tags/CAP/"/>
    
      <category term="BASE" scheme="http://jiezhi.github.io/tags/BASE/"/>
    
      <category term="ACID" scheme="http://jiezhi.github.io/tags/ACID/"/>
    
  </entry>
  
  <entry>
    <title>《Apache Hive Cookbook》读书笔记</title>
    <link href="http://jiezhi.github.io/2019/02/19/note-hive-cookbook/"/>
    <id>http://jiezhi.github.io/2019/02/19/note-hive-cookbook/</id>
    <published>2019-02-19T09:26:42.000Z</published>
    <updated>2019-03-08T10:00:09.997Z</updated>
    
    <content type="html"><![CDATA[<p>有 SQL 基础的话，差不多一天可以看完。<br>这里把我看这本书做的笔记分享一下（从 <a href="https://www.notion.so/?r=507f808e5cb544809dfe98d930ee2cae" target="_blank" rel="noopener">Notion</a> 里迁移出来）</p><a id="more"></a><img src="/2019/02/19/note-hive-cookbook/cover.jpg" title="cover"><h1 id="Developing-Hive"><a href="#Developing-Hive" class="headerlink" title="Developing Hive"></a>Developing Hive</h1><h2 id="Deploying-Hive-Metastore"><a href="#Deploying-Hive-Metastore" class="headerlink" title="Deploying Hive Metastore"></a>Deploying Hive Metastore</h2><ul><li>The Hive table and database definitions and mapping to the data in HDFS is stored in a metastore.</li><li>A metastore is a central repository for Hive metadata. Consists two main components:<ul><li>Services to which the client connects and queries the metastore</li><li>A backing database to store the metadata</li></ul></li><li><p>Configure:</p><ul><li>An <strong>embedded</strong> metastore</li><li>A <strong>local</strong> metastore</li><li><p>A <strong>remote</strong> metastore</p><img src="/2019/02/19/note-hive-cookbook/1.png" title="hive-service"><!-- ![](/hive-cookbook-hive-service.png) --><p>HIVE Service JVM</p></li></ul></li></ul><h1 id="Services-in-Hive"><a href="#Services-in-Hive" class="headerlink" title="Services in Hive"></a>Services in Hive</h1><h2 id="HiveServer2"><a href="#HiveServer2" class="headerlink" title="HiveServer2"></a>HiveServer2</h2><blockquote><p>HiveServer2 is an enhancement of HiveServer provided in earlier versions of Hive.HiveServer’s limitations of <strong>concurrency</strong> and <strong>authentication</strong> is resolved in HiveServer2. HiveServer2 is based on <strong>Thrift RPC</strong>.It supports multiple types of clients, including JDBC and ODBC.</p></blockquote><h2 id="HiveServer2-High-Availability"><a href="#HiveServer2-High-Availability" class="headerlink" title="HiveServer2 High Availability"></a>HiveServer2 High Availability</h2><ul><li><strong>ZooKeeper</strong></li></ul><h2 id="Hive-metastore-service"><a href="#Hive-metastore-service" class="headerlink" title="Hive metastore service"></a>Hive metastore service</h2><ul><li>In Hive, the data is stored in <strong>HDFS</strong> and the table, database, schema, and the HQL definitions are stored in a <strong>metastore</strong>.</li><li>The metastore could be any RDBMS database, such as MYSQL or Oracle.</li><li>Hive creates a database and a set of tables in metastore to store HiveQL definitions.</li></ul><h2 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a>Hue</h2><h1 id="Understanding-the-Hive-Data-Model"><a href="#Understanding-the-Hive-Data-Model" class="headerlink" title="Understanding the Hive Data Model"></a>Understanding the Hive Data Model</h1><h2 id="Intruduction"><a href="#Intruduction" class="headerlink" title="Intruduction"></a>Intruduction</h2><ul><li>Data types<ul><li>Primitive data types<ul><li>Numeric data types</li><li>String data types</li><li>Date/Time data type</li><li>Miscellaneous data types<ul><li>Boolean</li><li>Binary</li></ul></li></ul></li><li>Complex data types<ul><li>STRUCT</li><li>MAP</li><li>ARRAY</li></ul></li></ul></li></ul><h2 id="Partitioning"><a href="#Partitioning" class="headerlink" title="Partitioning"></a>Partitioning</h2><blockquote><p>Partitioning in Hive is used to increase query performance.</p></blockquote><ul><li><p><strong>Mnaged</strong> table</p><blockquote><p>In a managed table, if you delete a talbe, then the data of that table will also get deleted. Similarly, if you delete a partition, then the data of that partition will also get deleted.</p></blockquote><pre><code>CREATE TABLE customer(id STRING, name STRING, gender STRING) PARTITIONED BY (country STRING, state STRING);</code></pre><ul><li>By setting the value of <code>hive.mapred.mode</code> to strict, it will prevent running risky queries.</li><li>Loading data in a managed partitioned table<ul><li>Static Partitioning</li><li>Dynamic Partitioning</li></ul></li></ul></li><li><p><strong>External</strong> table</p><blockquote><p>Patitioning external tables works in the same way as in managed tables. Except this in the external table, when you delete a partition, the data file doesn’t get deleted.</p></blockquote></li></ul><h2 id="Bucketing"><a href="#Bucketing" class="headerlink" title="Bucketing"></a>Bucketing</h2><blockquote><p>Bucketing is a technique that allows you to decompose your data into more manageable parts, that is, fix the number of buckets.</p></blockquote><ul><li>Usually, partitioning provides a way of segregating the data of a Hive table into multiple files or directories.</li><li>Partitioning doesn’t perform well if there is a large number of partitions.</li><li><p>Bucketing concept is based on the hashing principle, where same type of keys are always sent to the same bucket.</p><p>  <code>Bucket number = hash_function(bucketing_column) mod num_buckets</code></p><p>  <code>set hive.enforce.bucketing=true;</code></p></li><li><p>Two bullet points:</p><ul><li>In partitioning, a column defined as a partitioned column is not included in a schema columns of a Hive table. But in bucketing, a column defined as a bucketed column is included in the schema columns of the Hive table.</li><li><p>We cannot use the <code>LOAD DATA</code> statement to load the data into the bucketed table as we do in partitioned table. Rather, we have to use the <code>INSERT</code> statements to insert data by selecting data from some other table.</p><h1 id="Hive-Data-Definition-Language-DDL"><a href="#Hive-Data-Definition-Language-DDL" class="headerlink" title="Hive Data Definition Language(DDL)"></a>Hive Data Definition Language(DDL)</h1><h2 id="Creating-tables"><a href="#Creating-tables" class="headerlink" title="Creating tables"></a>Creating tables</h2></li><li><p>The <code>LIKE</code> clause in a create table command creates a copy of an existing table with a different name and without the data. It just creates a structure like that of an existing table without copying its data.</p></li><li><p>Parameters:</p><ul><li>[TEMPORARY]</li><li>[EXTERNAL]</li><li>[IF NOT EXISTS]</li><li>[PARTITIONED BY]</li><li>[CLUSTERED BY]</li><li>[SKEWED BY]: 解决数据倾斜问题，较多的值被分隔成多个文件，其余的值分到其他文件中。</li><li>…</li></ul><h2 id="Creating-views"><a href="#Creating-views" class="headerlink" title="Creating views"></a>Creating views</h2><blockquote><p>A <code>view</code> is a virtual table that acts as a window to the dat for the underlying table commonly known as the <code>base</code> table. It consists of rows and columns but no physical data. So when a <code>view</code> is accessed, the underlying <code>base</code> table is queried for the output.</p></blockquote></li></ul></li></ul><h2 id="HCatalog"><a href="#HCatalog" class="headerlink" title="HCatalog"></a>HCatalog</h2><blockquote><p><code>HCatalog</code> is a storage management tool that enables framworks other than Hive to leverage a data model to read and write data. HCatalog tables provide an abstraction on the data format in HDFS and allow frameworks such as <code>PIG</code> and <code>MapReduce</code> to use the data without being concerned about the data format, such as <code>RC</code>, <code>ORC</code>, and text files.</p></blockquote><ul><li><code>HCatInputFormat</code> and <code>HCatOutputFormat</code>, which are the implementations of Hadoop <code>InputFormat</code> and <code>OutputFormat</code>, are the interfaces provided to <code>PIG</code> and <code>MapReduce</code>.</li></ul><h2 id="WebHCat"><a href="#WebHCat" class="headerlink" title="WebHCat"></a>WebHCat</h2><blockquote><p>WebHCat, formerly called Templeton, allow access to the HCatalog service using REST APIs.</p></blockquote><h1 id="Hive-Data-Manipulation-Language-DML"><a href="#Hive-Data-Manipulation-Language-DML" class="headerlink" title="Hive Data Manipulation Language(DML)"></a>Hive Data Manipulation Language(DML)</h1><h1 id="Hive-Extensibility-Features"><a href="#Hive-Extensibility-Features" class="headerlink" title="Hive Extensibility Features"></a>Hive Extensibility Features</h1><h2 id="Serialization-and-deserialization-formats-and-data-types"><a href="#Serialization-and-deserialization-formats-and-data-types" class="headerlink" title="Serialization and deserialization formats and data types"></a>Serialization and deserialization formats and data types</h2><ul><li>LazySimpleSerDe, the default SerDes format of Hive.</li><li>RegexSerDe</li><li>AvroSerDe</li><li>OrcSerDe</li><li>ParquetHiveSerDe</li><li>JSONSerDe</li><li>CSVSerDe</li></ul><h2 id="Exploring-views"><a href="#Exploring-views" class="headerlink" title="Exploring views"></a>Exploring views</h2><ul><li>A view is treated as a table in Hive</li></ul><h2 id="Exploring-indexes"><a href="#Exploring-indexes" class="headerlink" title="Exploring indexes"></a>Exploring indexes</h2><ul><li>Indexes are useful for increasing the performance of frequent queries based on certain columns.</li></ul><h2 id="Hive-partitioning"><a href="#Hive-partitioning" class="headerlink" title="Hive partitioning"></a>Hive partitioning</h2><ul><li>Static partitioning</li><li>Dynamic partitioning</li></ul><h2 id="Creating-buckets-in-Hive"><a href="#Creating-buckets-in-Hive" class="headerlink" title="Creating buckets in Hive"></a>Creating buckets in Hive</h2><pre><code>CREATE table sales_buck (id int, fname string, ...) clustered by (id) into 50 buckets row format delimited fieldsterminated by &apos;\t&apos;;set hive.enforce.bucketing=true;insert into table sales_buck select * from sales;</code></pre><h2 id="Analytics-functions-in-Hive"><a href="#Analytics-functions-in-Hive" class="headerlink" title="Analytics functions in Hive"></a>Analytics functions in Hive</h2><p><em>see page 122</em></p><ul><li><p>RANK</p><blockquote><p>It is similar to ROW_NUMBER, but the equal rows are ranked with the same number.</p></blockquote></li><li><p>DENSE_RANK</p><blockquote><p>In a normal RANK function, we see a gap between the numbers in rows. DENSE_RANK is a function with no gap.</p></blockquote></li><li><p>ROW_NUMBER</p><blockquote><p>This function will provide a unique number to each row in resultset based on the ORDER BY clause within the PARTITION.</p></blockquote></li><li><p>PERCENT_RANK</p><blockquote><p>It is very similar to the CUME_DIST function. It returns a value from 0 to 1 inclusive.</p></blockquote></li><li><p>CUME_DIST(Cumulative distribution)</p><blockquote><p>It computes the relative postion of a column value in a group.</p></blockquote></li><li><p>NTILE</p><blockquote><p>NTILE distributes the number of rows in a partition into a certain number of groups.</p></blockquote></li></ul><h2 id="Windowing-in-Hive"><a href="#Windowing-in-Hive" class="headerlink" title="Windowing in Hive"></a>Windowing in Hive</h2><p><em>see page 126</em></p><blockquote><p>Windowing in Hive allows an analyst to creagte a window of data to operate aggregation and other analytical functions, such as LEAD and LAG.</p></blockquote><ul><li><p>Partition specification</p><p>  It includes a column reference from the table. It could not be any aggregation or other window specification.</p></li><li><p>Order specification</p><p>  It comprises a combination of one or more columns.</p><ul><li><p>Handling NULLs</p><p>  There is no support for Nulls first or last specification. In Hive, Nulls are returned first.</p></li></ul></li><li><p>Window frame</p><p>  A frame has a start boundary and an optional end boundary:</p><ul><li>Frame Type<ul><li>ROW</li><li>RANGE</li></ul></li></ul></li><li>Frame boundary</li><li>Effective window frames</li><li>Source name for window definition</li><li><p>LEAD</p><blockquote><p>The LEAD function is used to return the data from the next set of rows.</p></blockquote></li><li><p>LAG</p><blockquote><p>The LAG function is used to return the data from the previous set of rows.</p></blockquote></li><li><p>FIRST_VALUE</p></li><li>LAST_VALUE</li></ul><h2 id="File-formats"><a href="#File-formats" class="headerlink" title="File formats"></a>File formats</h2><ul><li>TEXTFILE, default format</li><li><p>SEQUENCEFILE</p><p>  If you want to save disk storage while keeping large datasets</p></li><li><p>RCFILE</p><p>  Also known as <strong>Record Columnar File</strong>, stores data in a compressed format on the disk. It provides the following features of storage and processing optimization:</p><ul><li>Fast storage of data</li><li>Optimized storage utilization</li><li><p>Better query processing</p><p>The RCFILE format flattens the data in terms of both rows and columns. If you need a certain column for analytics, it would not scan the complete data; instead, it would return the required columns.</p></li></ul></li><li><p>ORC</p><p>  <strong>Optimized Row Columnar</strong></p><p>  This is a highly efficient way of storing and processing data in Hive. Data stored in the ORC format improves performance in reading, writing, and processing data with Hive.</p></li><li><p>PARQUET</p><p>  This is a column-oriented storage format that is efficient at querying particular columns in the table.</p></li><li><p>AVRO</p></li></ul><h1 id="Joins-and-Join-Optimization"><a href="#Joins-and-Join-Optimization" class="headerlink" title="Joins and Join Optimization"></a>Joins and Join Optimization</h1><h2 id="Using-a-skew-join"><a href="#Using-a-skew-join" class="headerlink" title="Using a skew join"></a>Using a skew join</h2><p>A skew join is used when there is a table with skew data in the joining column. A skew table is a table that is having values that are present in large numbers in the table compared to other data. Skew data is stored in a separate file while the rest of the data is stored in a separate file. </p><p>If there is a need to perform a join on a column of a table that is appearing quite often in the table, the data for that particular column will go to a single reducer, which will become a bottleneck while performing the join. To reduce this, a skew join is used.</p><h1 id="Statics-in-Hive"><a href="#Statics-in-Hive" class="headerlink" title="Statics in Hive"></a>Statics in Hive</h1><h2 id="Analyze"><a href="#Analyze" class="headerlink" title="Analyze"></a>Analyze</h2><pre><code>ANALYZE TABLE sales COMPUTE STATISTICS;</code></pre><h1 id="Functions-in-Hive"><a href="#Functions-in-Hive" class="headerlink" title="Functions in Hive"></a>Functions in Hive</h1><h1 id="Hive-Tuning"><a href="#Hive-Tuning" class="headerlink" title="Hive Tuning"></a>Hive Tuning</h1><h2 id="Enabling-predicate-pushdown-optimiztions-in-Hive"><a href="#Enabling-predicate-pushdown-optimiztions-in-Hive" class="headerlink" title="Enabling predicate pushdown optimiztions in Hive"></a>Enabling predicate pushdown optimiztions in Hive</h2><p>Predicate pushdown is a traditional RDBMS term, whereas in Hive, it works as predicate pushup.</p><p><code>hvie.optimize.ppd=true;</code></p><h2 id="Optimizations-to-reduce-the-number-of-map"><a href="#Optimizations-to-reduce-the-number-of-map" class="headerlink" title="Optimizations to reduce the number of map"></a>Optimizations to reduce the number of map</h2><h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><blockquote><p>Sampling in Hive is a way to wirte queries on a small chunk of data instead of the entire table.</p></blockquote><h1 id="Hive-Security"><a href="#Hive-Security" class="headerlink" title="Hive Security"></a>Hive Security</h1><h1 id="Hive-Integration-with-Other-Frameworks"><a href="#Hive-Integration-with-Other-Frameworks" class="headerlink" title="Hive Integration with Other Frameworks"></a>Hive Integration with Other Frameworks</h1><h2 id="Working-with-Apache-Spark"><a href="#Working-with-Apache-Spark" class="headerlink" title="Working with Apache Spark"></a>Working with Apache Spark</h2><h2 id="Working-with-Accumulo"><a href="#Working-with-Accumulo" class="headerlink" title="Working with Accumulo"></a>Working with Accumulo</h2><h2 id="Working-with-HBase-Google-Drill"><a href="#Working-with-HBase-Google-Drill" class="headerlink" title="Working with HBase (Google Drill)"></a>Working with HBase (Google Drill)</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有 SQL 基础的话，差不多一天可以看完。&lt;br&gt;这里把我看这本书做的笔记分享一下（从 &lt;a href=&quot;https://www.notion.so/?r=507f808e5cb544809dfe98d930ee2cae&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Notion&lt;/a&gt; 里迁移出来）&lt;/p&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://jiezhi.github.io/categories/Hive/"/>
    
    
      <category term="Note" scheme="http://jiezhi.github.io/tags/Note/"/>
    
      <category term="Hive" scheme="http://jiezhi.github.io/tags/Hive/"/>
    
      <category term="Big Data" scheme="http://jiezhi.github.io/tags/Big-Data/"/>
    
  </entry>
  
  <entry>
    <title>GraphQL简介</title>
    <link href="http://jiezhi.github.io/2018/06/10/graphql/"/>
    <id>http://jiezhi.github.io/2018/06/10/graphql/</id>
    <published>2018-06-10T07:02:51.000Z</published>
    <updated>2019-03-08T09:40:44.801Z</updated>
    
    <content type="html"><![CDATA[<p>GraphQL会是取代REST API的下一代标准么？</p><a id="more"></a><h1 id="什么是GraphQL"><a href="#什么是GraphQL" class="headerlink" title="什么是GraphQL"></a>什么是GraphQL</h1><p><img src="http://qn-cdn.nospider.net/2018-06-10-graphql.png" alt="graphql"></p><blockquote><p>GraphQL是比REST更高效、强大和灵活的新一代API标准。Facebook开发了GraphQL并且将其开源，目前其由一大群来自全球各地的公司和个人维护。</p></blockquote><p>注意到GraphQL是API标准，不要看到QL结尾就以为其是一种数据库技术。</p><h2 id="比REST更灵活的一种选择"><a href="#比REST更灵活的一种选择" class="headerlink" title="比REST更灵活的一种选择"></a>比REST更灵活的一种选择</h2><p>REST是目前比较流行的一种暴露服务端数据的常见方式，其简化了客户端尤其是移动端和服务器交互的流程。但是随着业务变得复杂，有些情况变得棘手：</p><ol><li><p>移动端数量的增多，对数据的效率要求变高<br>移动端和PC端相比，是需要提高对数据获取的效率的，这个效率就是说要减少网络请求、要减少无用数据的传输。</p></li><li><p>应对复杂的前端框架和平台<br>现在的情况是仅维护一套API来应对不同框架和平台的请求。PC端一个页面比移动端一个页面展示的内容要多很多，之前后端提供给PC端的API如果直接提供给移动端来使用势必造成资源浪费。所以移动端的人会去找后端的人干一架，结果要么是后端再给移动端单独写一套API，要么就是移动端忍受着API请求返回数据中存在大量冗余的数据。</p></li><li><p>需要更快速地迭代更新<br>互联网时代最大的特色除了加班也许就是快了。好多公司在喊着小步快跑、快速试错，毕竟市场不等人。然而REST标准的API似乎很难快速地跟上这快跑的节奏。也许一个API刚出来，产品那边已经改了原型，界面重新设计了。这时候就要麻烦后端同学加个班把接口改一下吧。</p></li></ol><h2 id="谁在用GraphQL"><a href="#谁在用GraphQL" class="headerlink" title="谁在用GraphQL"></a>谁在用GraphQL</h2><p>一个产品的流行，肯定是解决了目前的某些痛点。虽然GraqhQL目前在国内还不算流行，可是在美利坚已经有不少巨头在使用了：</p><h2 id><a href="#" class="headerlink" title></a><img src="http://qn-cdn.nospider.net/2018-06-10-15286168671382.jpg" alt></h2><h1 id="GraphQL-vs-REST"><a href="#GraphQL-vs-REST" class="headerlink" title="GraphQL vs REST"></a>GraphQL vs REST</h1><p>我们来看一下对于不同API标准下，从服务端获取数据的区别。比如在REST API标准下，有三个接口：</p><ul><li><p>/users/<id><br>该接口返回某用户基本信息</id></p></li><li><p>/users/<id>/posts<br>该接口返回某用户所有的文章</id></p></li><li><p>/users/<id>/followers<br>该接口返回某用户所有的关注者</id></p></li></ul><p><img src="http://qn-cdn.nospider.net/2018-06-10-15286174419940.jpg" alt></p><p><em>如图所示，要通过三个不同的请求才能获得某用户及其文章和关注者的信息，其中还存在很多不需要的信息。 </em></p><p>再看一下GraphQL API的实现：<br><img src="http://qn-cdn.nospider.net/2018-06-10-15286175800503.jpg" alt><br><em>客户端声明自己想要的信息，然后服务端根据请求返回相应的数据</em></p><p>目前可见的优点：</p><ol><li>避免了REST API中常见的信息过多或过少的问题<br>信息过多是指，接口中总会存在客户端不需要的信息，信息过少是指单条接口无法满足客户端需求，需要请求多个接口才能满足需要</li><li><p>前端可以快速迭代<br>在REST API中，一般都是后端定义好了API，返回固定的数据格式。当前端业务或需求发生变化时，后端很难跟上变动的节奏。如今，业务变化已经难以避免，所以当前端和后端都要相应地作出改动，这样效率势必降低。就我们公司业务来讲，很多情况下，前端一两天的改动如果再拉上后端，人多肯定要开会再加上沟通成本的问题，这个需求没个一周两周很难搞定。设想一下，如果在GraphQL标准下，除非大的改版，后端基本不用出人力来跟着一起需求评审，前端自己定义查询的内容就搞定了。</p></li><li><p>更深层次地进行分析<br>当客户端可以选择自己想请求数据的内容时，这时候就可以分析出哪些信息是用户感兴趣的，也可以更深层次地分析现有数据是如何被应用的。<br>此外，也可以分析出哪些信息用户不再感兴趣了。</p></li><li><p>Schema &amp; Type系统的优点<br>GraphQL使用一种强类型系统来定义API，所有的API都通过GraphQL模式定义语言（Schema Definition Language，SDL）来暴露类型数据。而这个模式就是服务端和客户端之间的协议，通过此模式我们可以知道服务端可以提供哪些数据，而客户端又可以获取哪些数据。</p></li></ol><p>一旦模式定义好了，前后端就可以据此独立进行开发了。</p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="模式定义语言（The-Schema-Definition-Language，SDL）"><a href="#模式定义语言（The-Schema-Definition-Language，SDL）" class="headerlink" title="模式定义语言（The Schema Definition Language，SDL）"></a>模式定义语言（The Schema Definition Language，SDL）</h2><p>比如这里定义了一个<code>Person</code>类型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">type Person &#123;</span><br><span class="line">  name: String!</span><br><span class="line">  age: Int!</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Person类型又两个字段，<code>name</code>和<code>age</code>，分别为<code>String</code>和<code>Int</code>类型。<code>!</code>表示该字段是必须的。</p><p>同时，两种类型之间可以有关联，比如<code>Person</code>可以和<code>Post</code>关联：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">type Post &#123;</span><br><span class="line">title: String!</span><br><span class="line">author: Person!</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>相应的，<code>Person</code>中也可以关联<code>Post</code>类型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type Person &#123;</span><br><span class="line">name: String!</span><br><span class="line">age: Int!</span><br><span class="line">posts: [Post!]!</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中<code>Person</code>和<code>Post</code>是一对多的关系</p><h2 id="通过Queries获取数据"><a href="#通过Queries获取数据" class="headerlink" title="通过Queries获取数据"></a>通过Queries获取数据</h2><p>在请求数据之前，你需要GraphQL-IDE来模拟下面的请求，有<a href="https://api.graph.cool/simple/v1/cjhvs1vtt4ahm012322aexl4n/?query=%7B%0A%20%20allPersons%20%7B%0A%20%20%20%20name%0A%20%20%7D%0A%7D" target="_blank" rel="noopener">Web版</a> 也有<br><a href="https://github.com/graphcool/graphql-playground" target="_blank" rel="noopener">客户端</a> </p><p>如果你选择了客户端，在启动页面打开这个地址： <a href="https://api.graph.cool/simple/v1/cjhvs1vtt4ahm012322aexl4n/" target="_blank" rel="noopener">https://api.graph.cool/simple/v1/cjhvs1vtt4ahm012322aexl4n/</a></p><p><img src="http://qn-cdn.nospider.net/2018-06-10-15286229140756.jpg" alt></p><p>让我们来一个最简单的请求：</p><p><img src="http://qn-cdn.nospider.net/2018-06-10-15286230297031.jpg" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  allPersons &#123;</span><br><span class="line">    name</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到返回的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;data&quot;: &#123;</span><br><span class="line">    &quot;allPersons&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;: &quot;Johnny&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;: &quot;Sarah&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;: &quot;Alice&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;: &quot;Bob&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;: &quot;Alice&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果我们想在结果里带上age的话：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  allPersons &#123;</span><br><span class="line">    name</span><br><span class="line">    age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://qn-cdn.nospider.net/2018-06-10-15286232389082.jpg" alt></p><h3 id="带参请求"><a href="#带参请求" class="headerlink" title="带参请求"></a>带参请求</h3><p>同样没问题，可以看到只返回了最后两条数据：<br><img src="http://qn-cdn.nospider.net/2018-06-10-15286233483934.jpg" alt></p><h2 id="修改数据"><a href="#修改数据" class="headerlink" title="修改数据"></a>修改数据</h2><p>这里说的修改包含三种操作：</p><ul><li>创建数据</li><li>更新数据</li><li>删除数据</li></ul><p>这里以创建数据为例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mutation &#123;</span><br><span class="line">  createPerson(name: &quot;Bob&quot;, age: 36) &#123;</span><br><span class="line">    name</span><br><span class="line">    age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://qn-cdn.nospider.net/2018-06-10-15286234981761.jpg" alt></p><h2 id="订阅实时更新"><a href="#订阅实时更新" class="headerlink" title="订阅实时更新"></a>订阅实时更新</h2><p>GraphQL提供了<code>subscriptions</code>的概念来提供给客户端订阅事件。当客户端订阅某事件后，其将会和服务端保持一个稳定的链接。当特定事件触发时，服务端会推送对应的数据给客户端。</p><p>比如我们想订阅新建<code>Person</code>类型事件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">subscription &#123;</span><br><span class="line">  newPerson &#123;</span><br><span class="line">    name</span><br><span class="line">    age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当有新的Person类型被创建时，客户端就能接收到更新的数据了。</p><h2 id="定义模式"><a href="#定义模式" class="headerlink" title="定义模式"></a>定义模式</h2><p>模式是GraphQL API中最重要概念之一，其定义了API能提供哪些数据，以及客户端如何获取这些数据。</p><p>有几个特殊的类型被称之为<code>root</code>类型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type Query &#123; ... &#125;</span><br><span class="line">type Mutation &#123; ... &#125;</span><br><span class="line">type Subscription &#123; ... &#125;</span><br></pre></td></tr></table></figure><p>本文中用到的完整模式定义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">type Query &#123;</span><br><span class="line">  allPersons(last: Int): [Person!]!</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Mutation &#123;</span><br><span class="line">  createPerson(name: String!, age: Int!): Person!</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Subscription &#123;</span><br><span class="line">  newPerson: Person!</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Person &#123;</span><br><span class="line">  name: String!</span><br><span class="line">  age: Int!</span><br><span class="line">  posts: [Post!]!</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Post &#123;</span><br><span class="line">  title: String!</span><br><span class="line">  author: Person!</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>至此，大家应该对GraphQL有一个感性的认识了吧。如果不想仅停留在概念层面，你可以到<a href="https://graphql.org/code/gg" target="_blank" rel="noopener">这里</a>找到目前已经实现的框架。（反正我已经用起来了:P）<br><img src="http://qn-cdn.nospider.net/2018-06-10-15286245421358.jpg" alt></p><p>更多内容可访问以下网站：</p><p><a href="https://www.graphql.org/" target="_blank" rel="noopener">https://www.graphql.org/</a><br><a href="https://www.howtographql.com/" target="_blank" rel="noopener">https://www.howtographql.com/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;GraphQL会是取代REST API的下一代标准么？&lt;/p&gt;
    
    </summary>
    
      <category term="GraphQL" scheme="http://jiezhi.github.io/categories/GraphQL/"/>
    
    
      <category term="Translate" scheme="http://jiezhi.github.io/tags/Translate/"/>
    
      <category term="GraphQL" scheme="http://jiezhi.github.io/tags/GraphQL/"/>
    
  </entry>
  
  <entry>
    <title>我与阅读</title>
    <link href="http://jiezhi.github.io/2018/03/25/book-app/"/>
    <id>http://jiezhi.github.io/2018/03/25/book-app/</id>
    <published>2018-03-25T14:03:38.000Z</published>
    <updated>2019-02-19T08:37:22.846Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下自己读书历程，并推荐一下自己用得比较多的阅读相关的APP。</p><a id="more"></a><p>从小到大就自诩自己是个喜欢阅读的人，一方面可能是我们家族在村子里算是有文化的人吧，遗传了一点（听说我爷爷的爷爷是私塾老师，现在大家子里当老师的也挺多的），另一方面可能因为小时候偏内向，所以较多的时候都是个安静的男孩子，就喜欢翻翻书什么的。</p><p>记得小学的时候就喜欢找那些初中生的那些阅读课本看，实在没得看就翻成语词典或新华字典了。到了中学，因为镇上没有书店，就经常坐公交去县城买书看。那时候公交车单程要3块钱，对于一周也才几块钱零花钱的我压力实在不小，然后就会顺路帮同学们买点资料什么的，顺便把路费给平摊了。那时候有个『龙门书局』的出版社，感觉挺酷的，就给自己起了个『星星书局』，并用橡皮刻了章，然后印在了每一本书上。</p><p>高中进了县城去了，高一时语文老师给我们解释了什么是『语文』：</p><blockquote><p>语为心声，文以载道</p></blockquote><p>正好那时候学校给每个年级配备了阅览室，里面有不少的杂志和文学书籍可以在里面看。那时候我下课就会跑过去，阅览室的老师对我这个为数不多的读者印象很好，就特批我可以把书带回去看，时间长了甚至让我帮她看着阅览室。这种有特权的感觉让我这个大男孩有点小骄傲，所以更是经常跑去借书看了。由于新校区才建立不久，所以没有图书馆。那时候最大的愿望就是能天天泡图书馆，听说大学里图书馆特别多，所以还是比较期待上大学的。</p><p>嗯，上了大学，还没拿到学生卡就开始往图书馆跑了，想着大学4年一定要看多少多少的书来着。天有不测风云，谁能想到，我看的书基本都是计算机类的书了。由于学校是医药类学校，虽然有信息学院，但可以肯定的是，真正对计算机感兴趣的实在不多。所以据我不完全观察，TP类书架大部分时间只有我一个人在那里看书。偶尔有几个妹子走那里停留，不用想是在找计算机等级考试的参考资料。就这样，在毕业的时候，学校第一次评选书虫奖，忘了自己读了多少书了，反正排在那一届第9名，平均一周一本书的进度吧（具体数据要等我找到当时的证书再修正）。</p><p>总体看来，对我影响比较大的老师基本都是语文老师，小学时被语文老师夸知道的成语多，然后我就使劲得翻成语词典；初中时被语文老师夸读的课外书多，我就使劲地找课外书读；高中时被语文老师夸…想不起来了，但是我使劲地背古诗词了，然后也写了几篇诗词，也模仿过汪曾祺先生写过几篇文章。至于大学时嘛，由于没有语文老师，然后我就基本上泡图书馆看计算机的书了。</p><p>对于用APP看书，一开始我是排斥的，觉得没有纸质书有感觉，也少了厚重感。那时候被人推荐关注了冯大辉，可能由于他也是医药类专业然后转行搞IT的原因吧。然后用上了他推荐了彼时还没被小米收购的『<strong>多看阅读</strong>』，在然后买了人生中第一本电子书<strong>《海底捞你学不会》</strong>：<br><img src="http://qn-cdn.nospider.net/2018-03-25-15219897449337.jpg" alt><br>然后这是当年的评价：<br><img src="http://qn-cdn.nospider.net/2018-03-25-15219898153947.jpg" alt><br>那时候才知道电子书除了txt还有epub格式的，还可以有像纸质书一样的排版，里面除了有文字还可以有图片甚至是视频。此后就走上了买电子书的不归路。</p><p>真正体会到电子书的好处，当然还是毕业后搬家了，有过几次搬家，没啥家当图书要占一大半，关键是看起来还不方便。后来我就逢人就推荐看电子书了，开始大部分人都和我当年一样觉得看纸质书有感觉，也以为电子书就是txt，也都不知道电子书是要买的。当然现在还是有不少人，即使工资已经很高了，说到电子书立马去找盗版资源。。。</p><p>废话不多说了，列举一些目前我在用的一些APP：</p><ul><li><p><a href="http://www.duokan.com/" target="_blank" rel="noopener">多看</a><br>说实话对多看是有特殊感情的，毕竟大部分书就是在多看买的，那时候多看还不是小米系的，那时候排版是最好的，那时候根本没有网文，那时候kindle还可以刷多看的。目前除了看《知乎周刊》和一些理财类的杂志，基本上就是看以前买的书了。</p></li><li><p><a href="https://du.163.com/invite?user=daa4ce39a9314eeca7e3ec58a28c9675&amp;bg=5&amp;from=singlemessage&amp;isappinstalled=0" target="_blank" rel="noopener">蜗牛读书</a></p></li></ul><blockquote><p>每天免费读书一小时</p></blockquote><p>后起之秀，通过每天免费1小时的创新吸引了不少人，里面也有不少的好书，比如中信出版社的。如果我想看的书在这里有的话就在这里看了，而且虽然每天只能免费1小时，但我相信能满足90%的人了。</p><ul><li><a href="https://weread.qq.com/" target="_blank" rel="noopener">微信阅读</a>  </li></ul><blockquote><p>微信读书让阅读不再孤独.</p></blockquote><p>微信出什么估计都能引起很大的轰动，腾讯利用熟人社交尝到了太多的甜头，微信阅读也不例外。之前多看也想基于阅读做关于读书的圈子，但没多久就做不下去了。对于微信阅读，刚推出的时候排版差、大量重复的段落等让我用了几天就抛之脑后了。但是等过一阶段再去看看，发现情况已经有了很大的改善。此外还有一个特点是每周可以用阅读时间来换取书币，每周最多10个吧。</p><p>上面这几个是目前用的比较多的，此外还有『京东阅读』、『百度阅读』等。</p><p>对于纸质书的话，也有不少APP可以记录阅读状态的，相当于在线书签了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下自己读书历程，并推荐一下自己用得比较多的阅读相关的APP。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Docker中Django处理消息队列遇到的坑</title>
    <link href="http://jiezhi.github.io/2018/01/05/django-with-mq-in-docker/"/>
    <id>http://jiezhi.github.io/2018/01/05/django-with-mq-in-docker/</id>
    <published>2018-01-05T01:35:47.000Z</published>
    <updated>2019-02-19T08:38:14.143Z</updated>
    
    <content type="html"><![CDATA[<p>早上过来发现昨天上线的代码还是有个问题，好在很快解决了，觉得有必要做个小总结了。</p><a id="more"></a><h4 id="python实现mq消息接收处理"><a href="#python实现mq消息接收处理" class="headerlink" title="python实现mq消息接收处理"></a>python实现mq消息接收处理</h4><h5 id="框架选择"><a href="#框架选择" class="headerlink" title="框架选择"></a>框架选择</h5><p>因为想不到怎么在Django里加上mq消息处理，所以就暴露出一个接口直接来调用。公司使用的是<a href="http://activemq.apache.org/" target="_blank" rel="noopener">activemq</a>，其支持4种协议：</p><blockquote><ul><li><a href="http://activemq.apache.org/openwire.html" target="_blank" rel="noopener">OpenWire</a> for high performance clients in Java, C, C++, C#</li><li><a href="http://activemq.apache.org/stomp.html" target="_blank" rel="noopener">Stomp</a> support so that clients can be written easily in C, Ruby, Perl, Python, PHP, ActionScript/Flash, Smalltalk to talk to ActiveMQ as well as any other popular Message Broker</li><li><a href="http://activemq.apache.org/amqp.html" target="_blank" rel="noopener">AMQP</a> v1.0 support</li><li><a href="http://activemq.apache.org/mqtt.html" target="_blank" rel="noopener">MQTT</a> v3.1 support allowing for connections in an IoT environment.</li></ul></blockquote><p>从中可以看到，最适合python的就是<a href="http://stomp.github.io/index.html" target="_blank" rel="noopener">Stomp</a>协议了。在<a href="http://stomp.github.io/implementations.html" target="_blank" rel="noopener">客户端列表</a>中可以找到不同实现语言对应的客户端，这里我选择了<a href="https://github.com/jasonrbriggs/stomp.py" target="_blank" rel="noopener">stomp.py</a>，谁让他排在搜索页面前面呢（其名称就是一种很好的SEO方式）。</p><h6 id="mq客户端的实现"><a href="#mq客户端的实现" class="headerlink" title="mq客户端的实现"></a>mq客户端的实现</h6><p>这里曾经遇到困扰好几天的坑：</p><ul><li><p><strong>协议的选择</strong></p><p>之前没接触过消息队列这块，天真地以为activemq就是mq的一种协议。豆油给我一个mq服务器地址和端口号（61616）后，使用stomp.py连接总是出错，连接时可以收到mq服务器返回的消息，解析却总是出错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread StompReceiverThread<span class="number">-1</span>:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"</span>, line <span class="number">916</span>, <span class="keyword">in</span> _bootstrap_inner</span><br><span class="line">    self.run()</span><br><span class="line">  File <span class="string">"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"</span>, line <span class="number">864</span>, <span class="keyword">in</span> run</span><br><span class="line">    self._target(*self._args, **self._kwargs)</span><br><span class="line">  File <span class="string">"path/of/project/lib/python3.6/site-packages/stomp.py-4.1.19-py3.6.egg/stomp/transport.py"</span>, line <span class="number">332</span>, <span class="keyword">in</span> __receiver_loop</span><br><span class="line">    f = utils.parse_frame(frame)</span><br><span class="line">  File <span class="string">"path/of/project/lib/python3.6/site-packages/stomp.py-4.1.19-py3.6.egg/stomp/utils.py"</span>, line <span class="number">138</span>, <span class="keyword">in</span> parse_frame</span><br><span class="line">    preamble = decode(frame[<span class="number">0</span>:preamble_end])</span><br><span class="line">  File <span class="string">"path/of/project/lib/python3.6/site-packages/stomp.py-4.1.19-py3.6.egg/stomp/backward3.py"</span>, line <span class="number">29</span>, <span class="keyword">in</span> decode</span><br><span class="line">    <span class="keyword">return</span> byte_data.decode()</span><br><span class="line">UnicodeDecodeError: <span class="string">'utf-8'</span> codec can<span class="string">'t decode byte 0xf0 in position 0: invalid continuation byte</span></span><br></pre></td></tr></table></figure><p>​</p><p>调试几天都是不行，就去提了个<a href="https://github.com/jasonrbriggs/stomp.py/issues/177" target="_blank" rel="noopener">issue</a>。</p><p>后来才发现是activemq实现了四种协议的服务，而不同协议开放的端口号不一样。stomp默认端口号为<strong>61613</strong>，端口号一换立马可以接收到消息。连接问题解决。</p><p>​</p></li><li><p><strong>如何加入到Django里</strong></p><p>被这个问题也是困扰了好久，单独的一个脚本到底如何加入到django里。曾经想过在启动django里随即运行该脚本，却一直找不到方法。因为我想把更多的精力放到对客户分数的处理上，而不是花太多的时间来处理后端的问题。所以比较急着想把这个问题解决，然而越急越无法找到实现的办法，也舍不得花时间来思考是不是这条路是不是对的路。正所谓<strong>我们都在不断赶路忘记了出路</strong>。</p><p>久久无果后，索性第一个版本就没加入消息队列的处理，回头处理数据去！</p><p>这两天忽然想到要不就把接收消息的代码单独拎出来运行，接收到消息就直接调用本地接口就可以了。然后直接用python启动就好了，调用本地接口也OK。</p><p><strong>有时候，被一个问题困扰太久就容易陷进去，不可自拔。</strong></p></li><li><p><strong>又出问题了</strong></p><p>然后把这个接收消息的脚本scp到服务器再运行又出现了2个问题：</p><ol><li>服务器没有python3</li><li>还要安装各种依赖库（会出现各种问题）</li></ol><p>没办法要为这个单独的脚本制作一个docker镜像了，有点高射炮打蚊子的感觉。但是好在可以做到平台无关性，不用去解决各种依赖的问题。</p><p>折腾一通后，可以正常启动运行了。然而天有不测风云，在本地可以正常运行的脚本，到了这里却出错了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; r = requests.delete(<span class="string">'http://0.0.0.0:8000/credit/apply-del/ED201******53'</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"/usr/local/lib/python3.6/site-packages/urllib3/connection.py"</span>, line <span class="number">141</span>, <span class="keyword">in</span> _new_conn</span><br><span class="line">    (self.host, self.port), self.timeout, **extra_kw)</span><br><span class="line">  File <span class="string">"/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py"</span>, line <span class="number">83</span>, <span class="keyword">in</span> create_connection</span><br><span class="line">    <span class="keyword">raise</span> err</span><br><span class="line">  File <span class="string">"/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py"</span>, line <span class="number">73</span>, <span class="keyword">in</span> create_connection</span><br><span class="line">    sock.connect(sa)</span><br><span class="line">ConnectionRefusedError: [Errno <span class="number">111</span>] Connection refused</span><br></pre></td></tr></table></figure><p>悲剧（被拒）了。</p><p>一开始以为是不是iptables配置的问题，可是没听说过iptables用来防本地访问的呀，在终端里用curl执行了一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X <span class="string">"DELETE"</span> <span class="string">"http://0.0.0.0:8000/credit/apply-del/ED201******53"</span></span><br><span class="line">&#123;<span class="string">"detail"</span>:<span class="string">"Not found."</span>,<span class="string">"status_code"</span>:404&#125;</span><br></pre></td></tr></table></figure><p>没问题（忽略404），也就是不是防火墙的问题之类的。又怀疑是requests库的问题，又进这个镜像里用python3自带的urllib.request执行也是被拒。正想着难道非要执行curl命令才行？不合常理呀。</p><p>这时候机智的我灵光一现，难不成是在docker里运行的问题？docker就算是一个轻量级的虚拟机了，网络应该默认是<strong>bridge</strong>形式的。</p><p>嗯，改为<strong>–net=host</strong>，搞定！</p></li></ul><h3 id="最后附上处理消息队列的脚本-关键地方已经打码-："><a href="#最后附上处理消息队列的脚本-关键地方已经打码-：" class="headerlink" title="最后附上处理消息队列的脚本(关键地方已经打码)："></a>最后附上处理消息队列的脚本(关键地方已经打码)：</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on 04/12/2017</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: 'Jiezhi.G@gmail.com'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Reference:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> json <span class="keyword">import</span> JSONDecodeError</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> stomp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(filename=<span class="string">'credit_message.log'</span>, level=logging.DEBUG, format=<span class="string">'%(asctime)s %(message)s'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListener</span><span class="params">(stomp.ConnectionListener)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_error</span><span class="params">(self, headers, body)</span>:</span></span><br><span class="line">        logging.error(<span class="string">'received an error "%s"'</span> % body)</span><br><span class="line">        print(<span class="string">'received an error "%s"'</span> % body)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_message</span><span class="params">(self, headers, body)</span>:</span></span><br><span class="line">        logging.info(<span class="string">'\n'</span>)</span><br><span class="line">        logging.info(<span class="string">'received a message "%s"'</span> % body)</span><br><span class="line">        print(<span class="string">'received a message "%s"'</span> % body)</span><br><span class="line">        <span class="comment"># print(body['apply_id'])</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = json.loads(body)</span><br><span class="line">            <span class="keyword">if</span> data[<span class="string">'applyId'</span>]:</span><br><span class="line">                delete_url = <span class="string">'http://0.0.0.0:8000/credit/apply-del/'</span> + data[<span class="string">'applyId'</span>]</span><br><span class="line">                r = requests.delete(delete_url)</span><br><span class="line">                print(r.status_code)</span><br><span class="line">                print(r.content)</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            logging.error(<span class="string">'KeyError process error: %s'</span> % body)</span><br><span class="line">        <span class="keyword">except</span> JSONDecodeError:</span><br><span class="line">            logging.error(<span class="string">'JSONDecodeError process error: %s'</span> % body)</span><br><span class="line">            print(<span class="string">'error:'</span>, body)</span><br><span class="line">  </span><br><span class="line">        logging.info(<span class="string">'-'</span> * <span class="number">80</span>)</span><br><span class="line">        logging.info(<span class="string">'\n'</span>)</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">80</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_connected</span><span class="params">(self, headers, body)</span>:</span></span><br><span class="line">        logging.info(<span class="string">'Connected'</span>)</span><br><span class="line">        print(<span class="string">'Connected'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_connecting</span><span class="params">(self, host_and_port)</span>:</span></span><br><span class="line">        logging.info(<span class="string">'Connecting'</span>)</span><br><span class="line">        print(<span class="string">'connecting'</span>, host_and_port)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_disconnected</span><span class="params">(self)</span>:</span></span><br><span class="line">        logging.info(<span class="string">'disconnected'</span>)</span><br><span class="line">        print(<span class="string">'disconnected'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_heartbeat</span><span class="params">(self)</span>:</span></span><br><span class="line">        logging.info(<span class="string">'heartbeat'</span>)</span><br><span class="line">        print(<span class="string">'heartbeat'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect_mq_server</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># conn = stomp.Connection([('127.0.0.1', 61616)])</span></span><br><span class="line">    <span class="comment"># conn = stomp.Connection11([('192.168.*.*', 61613)])</span></span><br><span class="line">    conn = stomp.Connection11([(<span class="string">'*.1.*.2'</span>, <span class="number">61613</span>)])</span><br><span class="line">    conn.set_listener(<span class="string">''</span>, MyListener())</span><br><span class="line">    conn.start()</span><br><span class="line">  </span><br><span class="line">    conn.connect(<span class="string">'admin'</span>, <span class="string">'password'</span>, wait=<span class="keyword">True</span>)</span><br><span class="line">    conn.subscribe(destination=<span class="string">'queue.bc.rgb.cs.commit.risk'</span>,</span><br><span class="line">                   id=<span class="string">'1'</span>,</span><br><span class="line">                   ack=<span class="string">'auto'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># conn.send(body='Hello world', destination='/queue/test')</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># conn.disconnect()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    connect_mq_server()</span><br></pre></td></tr></table></figure><p>  以及启动脚本：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">docker run \</span><br><span class="line">           -d \</span><br><span class="line">           --name credit_mq \</span><br><span class="line">           --net=host \</span><br><span class="line">           -v /home/docker/credit_message.log:/app/credit_message.log \</span><br><span class="line">           credit_mq:v1 \</span><br><span class="line">           python handle_message_queue.py</span><br></pre></td></tr></table></figure><p>  算了Dockerfile也放出来吧：</p>  <figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir /app</span></span><br><span class="line"><span class="bash">WORKDIR /app</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">COPY requirements.txt /app/requirements.txt</span></span><br><span class="line"><span class="bash">RUN pip install -r requirements.txt --trusted-host pypi.douban.com -i http://pypi.douban.com/simple</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">COPY . /app</span></span><br><span class="line"><span class="bash">CMD python handle_message_queue.py</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;早上过来发现昨天上线的代码还是有个问题，好在很快解决了，觉得有必要做个小总结了。&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://jiezhi.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://jiezhi.github.io/tags/python/"/>
    
      <category term="Docker" scheme="http://jiezhi.github.io/tags/Docker/"/>
    
      <category term="Django" scheme="http://jiezhi.github.io/tags/Django/"/>
    
      <category term="mq" scheme="http://jiezhi.github.io/tags/mq/"/>
    
      <category term="stomp" scheme="http://jiezhi.github.io/tags/stomp/"/>
    
  </entry>
  
  <entry>
    <title>2017年总结</title>
    <link href="http://jiezhi.github.io/2018/01/01/2017-summary/"/>
    <id>http://jiezhi.github.io/2018/01/01/2017-summary/</id>
    <published>2018-01-01T13:48:41.000Z</published>
    <updated>2019-03-08T10:06:49.575Z</updated>
    
    <content type="html"><![CDATA[<p>一年一度的总结来了，<del>赶在了2018年到来之前</del>。</p><p>总的来说今年是比较曲折的一年，体会了得与失，也是工作转型的一年。<br><a id="more"></a></p><h1 id="2017年个人总结"><a href="#2017年个人总结" class="headerlink" title="2017年个人总结"></a>2017年个人总结</h1><h2 id="计划完成情况"><a href="#计划完成情况" class="headerlink" title="计划完成情况"></a>计划完成情况</h2><p>去年的总结写得有点迟，都忘记给自己定计划了。这里翻出了<a href="http://jiezhi.github.io/2015/12/28/2015-summary/"><del>15年</del>14年</a>给自己定的计划：</p><blockquote><ol><li>一个月一本书（<em>非技术类</em>）</li><li>坚持锻炼（<em>起码隔三差五</em>）</li><li>坚持记账</li></ol></blockquote><h3 id="一个月一本书"><a href="#一个月一本书" class="headerlink" title="一个月一本书"></a>一个月一本书</h3><p>由于沉迷王者荣耀，一个月一本非技术类的书是远没有达到，这里手指掰一下也就基本《<a href="https://book.douban.com/subject/6811366/" target="_blank" rel="noopener">禅与摩托车维修艺术</a>》、《<a href="https://book.douban.com/subject/1255500/" target="_blank" rel="noopener">动物农场</a>》、《<a href="https://book.douban.com/subject/25902942/" target="_blank" rel="noopener">文明之光（第一册）</a>》、《<a href="https://book.douban.com/subject/26904421/" target="_blank" rel="noopener">征信与大数据</a>》、《<a href="https://book.douban.com/subject/27004031/" target="_blank" rel="noopener">蚂蚁金服</a>》。</p><p>这里自我批评一下，差点上了『奶头乐』的当，还好现在对王者荣耀没那么大的兴趣了，现在比较火的吃鸡游戏也没兴趣，总算是可以挤出时间继续看书了（在内心里摸了摸还在办公桌上吃灰的Kindle）。</p><h3 id="坚持锻炼"><a href="#坚持锻炼" class="headerlink" title="坚持锻炼"></a>坚持锻炼</h3><p>这个似乎从15年就开始松懈了，天天加班到吐再也没心思想什么锻炼的事，所以拖到了现在的公司，天不冷的时候基本会打一两次的篮球。</p><p>到9月份的时候开始和小伙伴下班后绕着园区一起跑步，然后先后相约参加了南京马拉松迷你跑、无锡半程马拉松和苏州半程马拉松。</p><p>此外，近两个月也去了几次健身房做一些力量型的训练。</p><p>总体来说，锻炼方面比之前有长进，希望可以再接再厉。</p><h3 id="坚持记账"><a href="#坚持记账" class="headerlink" title="坚持记账"></a>坚持记账</h3><p>这个的话，坚持有3年了，但是基本是在月底花一个晚上汇总所有的账单。仅仅是记录但是并没有复盘，也没有及时更新预算表，所以那只是一堆数据放在那里，并没有起到太大的作用。所以准备定期复盘。</p><hr><h2 id="学习和工作"><a href="#学习和工作" class="headerlink" title="学习和工作"></a>学习和工作</h2><h3 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h3><p>一直鞭策自己要成为一个持续学习者，今年坚持了每天都学英语：</p><p>在『百词斩』上背完了<strong>雅思核心单词</strong>，扇贝打卡<strong>365天</strong>。</p><p>当然最近学习的中心放在了机器学习方面，numpy、pandas、matplotlib、scikit、tensorflow都在马不停蹄地学习中。要理解好多的算法还是比较头疼的。</p><p>学完了Andrew Ng的机器学习的课程。</p><h3 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h3><p>先看自己<a href="http://jiezhi.github.io/2015/12/28/2015-summary/#%E5%B7%A5%E4%BD%9C">15年的总结</a>：</p><blockquote><p>不得不提的一个感想是，谁也不知道你之前学到的东西就在某天就用上了。在前一家公司下班后吃过饭就开始在<a href="http://jiezhi.github.io/2015/12/28/2015-summary/www.imooc.com">慕课网</a>和<a href="http://jiezhi.github.io/2015/12/28/2015-summary/study.163.com">网易云课堂</a>上学习,期间把python算是入门了，没想到半年后的项目中有很多用到python分析程序的，期间也写了很多脚本。那段时间心里老是念叨『但行好事，莫问前程』，虽不贴切，但我就是念叨这句。<strong>谁知道后面我会不会用python来分析『大』数据或搭建网站呢。</strong>（<em>高呼『生命苦短，我用Python』</em>）</p></blockquote><p>翻到这里，不禁虎躯一震，说得太对了。</p><p>目前公司的业务场景不太需要在移动端进行复杂的运算，所以很多任务都可以用Hybrid方式来开发，一套代码即可。原生开发似乎有点奢侈了，所以在下半年我被单独安排了做<strong>数据分析</strong>这块。</p><p>当然这也正合我意，本来做Android原生开发这块就已经有危机意识了，所以自己本身也在寻求其他的可能。</p><p>期间也第一次正式用Django搭建了一个服务，此前在自己的服务器上随便搭过基于Django的工具类网站。但这倒是第一次用在生产环境，可喜可贺。</p><hr><h2 id="其它关键词"><a href="#其它关键词" class="headerlink" title="其它关键词"></a>其它关键词</h2><ul><li>Macbook Pro 15寸</li><li>iPhone 8 Plus</li><li>iWatch S2</li><li>最强王者</li><li>分手</li><li>复合</li></ul><hr><h2 id="2018年目标"><a href="#2018年目标" class="headerlink" title="2018年目标"></a>2018年目标</h2><ul><li>首付</li><li>结婚</li><li>全马</li><li>67.5kg</li><li>英语/day</li><li>锻炼/week</li><li>一本书/month</li><li>在数据分析上做出点成绩/year</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一年一度的总结来了，&lt;del&gt;赶在了2018年到来之前&lt;/del&gt;。&lt;/p&gt;
&lt;p&gt;总的来说今年是比较曲折的一年，体会了得与失，也是工作转型的一年。&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://jiezhi.github.io/categories/Life/"/>
    
    
      <category term="2017" scheme="http://jiezhi.github.io/tags/2017/"/>
    
      <category term="总结" scheme="http://jiezhi.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>iPython技巧</title>
    <link href="http://jiezhi.github.io/2017/10/18/ipython/"/>
    <id>http://jiezhi.github.io/2017/10/18/ipython/</id>
    <published>2017-10-18T07:43:12.000Z</published>
    <updated>2019-02-19T08:49:19.808Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下ipython的一些技巧</p><a id="more"></a><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="Introspection"><a href="#Introspection" class="headerlink" title="Introspection"></a>Introspection</h3><ul><li>在对象前或对象后使用问号 <strong>?</strong> ，将会展示该对象的一些基本信息</li><li>如果在一个方法名上使用问号，将会展示该方法的说明文档（docstring）</li><li>使用双问号??将会展示该方法的源码</li><li><p>?和通配符*的搭配使用有奇效：</p><p>  In [100]:np.<em>load</em>?<br>  np.<strong>loader</strong><br>  np.load<br>  np.loads<br>  np.loadtxt<br>  np.pkgload</p></li></ul><h3 id="run-命令"><a href="#run-命令" class="headerlink" title="%run 命令"></a>%run 命令</h3><ul><li>该命令可以在IPython中直接执行Python程序文件。</li></ul><h3 id="paste"><a href="#paste" class="headerlink" title="%paste"></a>%paste</h3><ul><li>执行剪贴板中的代码</li></ul><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><p><img src="https://static.notion-static.com/1a3e46659f244dea86a85776cb6e6799/Untitled" alt></p><p><img src="https://static.notion-static.com/64b10bbc0c644bbaa4c356fa803c87ee/Untitled" alt></p><p>参考</p><ul><li>《Python for Data Analysis》Chapter 3</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下ipython的一些技巧&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Android中app.build配置</title>
    <link href="http://jiezhi.github.io/2017/10/18/gradle-in-android/"/>
    <id>http://jiezhi.github.io/2017/10/18/gradle-in-android/</id>
    <published>2017-10-18T07:40:29.000Z</published>
    <updated>2019-02-19T08:39:01.206Z</updated>
    
    <content type="html"><![CDATA[<p>备份一下项目中的app.build文件<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">release &#123;</span><br><span class="line">       minifyEnabled <span class="keyword">true</span></span><br><span class="line">       shrinkResources <span class="keyword">true</span></span><br><span class="line">       <span class="function">proguardFiles <span class="title">getDefaultProguardFile</span><span class="params">(<span class="string">'proguard-android.txt'</span>)</span>, 'proguard-rules.pro'</span></span><br><span class="line"><span class="function">       resValue "string", "app_name", "AppName"</span></span><br><span class="line"><span class="function">       resValue "string", "account_type", "io.jiezhi.app.type"</span></span><br><span class="line"><span class="function">   &#125;</span></span><br><span class="line"><span class="function">   preRelease </span>&#123;</span><br><span class="line">       debuggable <span class="keyword">true</span></span><br><span class="line">       jniDebuggable <span class="keyword">true</span></span><br><span class="line">       minifyEnabled <span class="keyword">true</span></span><br><span class="line">       zipAlignEnabled <span class="keyword">true</span></span><br><span class="line">       <span class="function">proguardFiles <span class="title">getDefaultProguardFile</span><span class="params">(<span class="string">'proguard-android.txt'</span>)</span>, 'proguard-rules.pro'</span></span><br><span class="line"><span class="function">       applicationIdSuffix ".pre"</span></span><br><span class="line"><span class="function">       resValue "string", "app_name", "AppName（Pre）"</span></span><br><span class="line"><span class="function">       resValue "string", "account_type", "io.jiezhi.app.type.pre"</span></span><br><span class="line"><span class="function">       versionNameSuffix '-pre'</span></span><br><span class="line"><span class="function">   &#125;</span></span><br><span class="line"><span class="function">   debug </span>&#123;</span><br><span class="line">       applicationIdSuffix <span class="string">".debug"</span></span><br><span class="line">       resValue <span class="string">"string"</span>, <span class="string">"app_name"</span>, <span class="string">"AppName（debug）"</span></span><br><span class="line">       resValue <span class="string">"string"</span>, <span class="string">"account_type"</span>, <span class="string">"io.jiezhi.app.type.debug"</span></span><br><span class="line">       versionNameSuffix <span class="string">'-debug'</span></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;备份一下项目中的app.build文件&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用注解改进Android代码</title>
    <link href="http://jiezhi.github.io/2017/05/07/annotations/"/>
    <id>http://jiezhi.github.io/2017/05/07/annotations/</id>
    <published>2017-05-07T12:38:31.000Z</published>
    <updated>2019-02-19T08:37:08.573Z</updated>
    
    <content type="html"><![CDATA[<p>注解有很多用途。 但是，在这里我们将讨论如何使用注解来改进我们的android编码。<br><a id="more"></a><br><img src="https://cdn-images-1.medium.com/max/800/1*gxjdLI-WsXSPWPvAyBAMaQ.png" alt></p><h2 id="注解即元数据"><a href="#注解即元数据" class="headerlink" title="注解即元数据"></a>注解即元数据</h2><p>而元数据是提供有关其他数据的信息的一组数据。</p><p>注解有很多用途。 但是，在这里我们将讨论如何使用注解来改进我们的android编码。</p><p>官方Android已经提供了支持注解，你可以添加如下依赖关系来导入注解：</p><blockquote><p>compile ‘com.android.support:support-annotations:x.x.x’</p></blockquote><p>每个人都想成为一个好的程序员，每天我也在努力地提高自己。</p><blockquote><p>任何人都可以编写计算机可以理解的代码，而好的程序员编写人可以理解的代码。 - Martin Fowler</p></blockquote><h2 id="让我们一起探索下一些有用的注解"><a href="#让我们一起探索下一些有用的注解" class="headerlink" title="让我们一起探索下一些有用的注解"></a>让我们一起探索下一些有用的注解</h2><h3 id="空类型注解"><a href="#空类型注解" class="headerlink" title="空类型注解"></a>空类型注解</h3><p><strong>@Nullable</strong>和<strong>@NonNull</strong>注解用于检查给定的变量、参数甚至是返回值是否为空。</p><p><strong>@Nullable</strong>：它表示一个变量、参数或返回值可以为空。</p><p><strong>@NonNUll</strong>：它表示不能为空的变量、参数或返回值。</p><p>例如：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@NonNull</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> View <span class="title">getView</span><span class="params">(@Nullable String s1, @NonNull String s2)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// s1 可以为空</span></span><br><span class="line">  <span class="comment">// s2 不可以为空</span></span><br><span class="line">  <span class="comment">// 该方法必须返回一个不为空的view</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以当我们尝试这样调用该方法时：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">View view = getView(<span class="string">"Amit"</span>, <span class="keyword">null</span>);</span><br></pre></td></tr></table></figure><p>在代码检测期间，Android Studio会警告你s2的值不能为空。</p><h2 id="资源注解"><a href="#资源注解" class="headerlink" title="资源注解"></a>资源注解</h2><p>我们都知道，Android对资源的引用（如drawable和string资源）都是传递int类型的，因此我们必须验证资源类型。假设代码中希望传入特定类型的资源（例如Drawables），这里可以传入引用类型的<strong>int</strong>值，但实际上却传入了另外类型的资源，例如R.string资源：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setText</span><span class="params">(@StringRes <span class="keyword">int</span> resId)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// resId 必须为string类型的id</span></span><br><span class="line">  <span class="comment">// resId 不能为常规的int值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以，当你像如下方式调用该方法时：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textView.setText(<span class="number">56</span>);</span><br></pre></td></tr></table></figure><p>在代码检测过程中，当传入非string资源的参数时，注解将会生成一个警告。</p><h2 id="线程注解"><a href="#线程注解" class="headerlink" title="线程注解"></a>线程注解</h2><p>线程注解检查方法是否在其期望的线程被调用。</p><p>支持的注解有：</p><p><strong>@MainThread</strong><br><strong>@UiThread</strong><br><strong>@WorkerThread</strong><br><strong>@BinderThread</strong><br><strong>@AnyThread</strong></p><p>如果你添加如下的注解：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WorkerThread</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomething</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="comment">// 该方法一定要从worker线程调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>如果你从非worker线程调用该方法时，你将会得到一个警告。</p><h2 id="值限定注解"><a href="#值限定注解" class="headerlink" title="值限定注解"></a>值限定注解</h2><p>有时候，我们必须对参数做一些约束，所以使用<strong>@IntRange</strong>，<strong>@FloatRange</strong>和<strong>@Size</strong>注解来验证传入参数的值。</p><p>当调用该方法的人可能会传递错误的值（超出指定的范围）时，该注解是非常有用的。</p><p>在下面的例子中，@IntRange注解确保传递的整数值必须在0到255之间。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAlpha</span><span class="params">(@IntRange(from=<span class="number">0</span>,to=<span class="number">255</span>)</span> <span class="keyword">int</span> alpha) </span>&#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="权限注解"><a href="#权限注解" class="headerlink" title="权限注解"></a>权限注解</h2><p>使用<strong>@RequiresPermission</strong>注解来验证调用者的权限。</p><p>以下示例注解<strong>setWallpaper()</strong>方法来确保方法的调用者具有<strong>permission.SET_WALLPAPERS</strong>权限：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RequiresPermission</span>(Manifest.permission.SET_WALLPAPER)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">setWallpaper</span><span class="params">(Bitmap bitmap)</span> <span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure><p>如果在调用该方法时没有在manifest文件中添加需要的权限，代码检测器将会给你一个警告。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;注解有很多用途。 但是，在这里我们将讨论如何使用注解来改进我们的android编码。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
